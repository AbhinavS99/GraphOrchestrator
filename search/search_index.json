{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GraphOrchestrator","text":"<p>GraphOrchestrator is a flexible Python library for building and executing graph-based workflows. It supports conditional routing, retries, human-in-the-loop nodes, and external tool integrations.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\udd04 Dynamic and conditional graph execution</li> <li>\ud83d\udd27 Built-in node types (Processing, Aggregator, Tool, AI, HITL)</li> <li>\ud83e\udde0 AI integration and fallback logic</li> <li>\u267b\ufe0f Retry, checkpointing, and parallel execution</li> <li>\ud83c\udf10 Remote tool servers via FastAPI</li> </ul> <p>View API Reference</p>"},{"location":"reference/Graph/","title":"Graph","text":""},{"location":"reference/Graph/#graphorchestrator.graph.graph","title":"<code>graphorchestrator.graph.graph</code>","text":""},{"location":"reference/Graph/#graphorchestrator.graph.graph.Graph","title":"<code>Graph</code>","text":"<p>Represents a directed graph composed of nodes and edges.</p> <p>Attributes:</p> Name Type Description <code>nodes</code> <code>Dict[str, Node]</code> <p>A dictionary mapping node IDs to Node objects.</p> <code>concrete_edges</code> <code>List[ConcreteEdge]</code> <p>A list of concrete edges in the graph.</p> <code>conditional_edges</code> <code>List[ConditionalEdge]</code> <p>A list of conditional edges in the graph.</p> <code>start_node</code> <code>Node</code> <p>The starting node of the graph.</p> <code>end_node</code> <code>Node</code> <p>The ending node of the graph.</p> Source code in <code>graphorchestrator\\graph\\graph.py</code> <pre><code>class Graph:\n    \"\"\"\n    Represents a directed graph composed of nodes and edges.\n\n    Attributes:\n        nodes (Dict[str, Node]): A dictionary mapping node IDs to Node objects.\n        concrete_edges (List[ConcreteEdge]): A list of concrete edges in the graph.\n        conditional_edges (List[ConditionalEdge]): A list of conditional edges in the graph.\n        start_node (Node): The starting node of the graph.\n        end_node (Node): The ending node of the graph.\n    \"\"\"\n\n    def __init__(\n        self, start_node: Node, end_node: Node, name: Optional[str] = \"graph\"\n    ) -&gt; None:\n        \"\"\"\n        Initializes a Graph object.\n\n        Args:\n            start_node (Node): The starting node of the graph.\n            end_node (Node): The ending node of the graph.\n            name (Optional[str]): An optional name for the graph (default: \"graph\").\n\n        Raises:\n            TypeError: If start_node or end_node is not of type Node.\n\n        Returns:\n            None\n        \"\"\"\n        self.nodes: Dict[str, Node] = {}\n        if not isinstance(start_node, Node) or not isinstance(end_node, Node):\n            raise TypeError(\"start_node and end_node must be of type Node\")\n\n        self.concrete_edges: List[ConcreteEdge] = []\n        self.conditional_edges: List[ConditionalEdge] = []\n        self.start_node = start_node\n        self.end_node = end_node\n\n        GraphLogger.get().info(\n            **wrap_constants(\n                message=\"Graph initialized\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"graph_initialized\",\n                    LC.CUSTOM: {\n                        \"start_node_id\": start_node.node_id,\n                        \"end_node_id\": end_node.node_id,\n                    },\n                }\n            )\n        )\n</code></pre>"},{"location":"reference/Graph/#graphorchestrator.graph.graph.Graph.__init__","title":"<code>__init__(start_node, end_node, name='graph')</code>","text":"<p>Initializes a Graph object.</p> <p>Parameters:</p> Name Type Description Default <code>start_node</code> <code>Node</code> <p>The starting node of the graph.</p> required <code>end_node</code> <code>Node</code> <p>The ending node of the graph.</p> required <code>name</code> <code>Optional[str]</code> <p>An optional name for the graph (default: \"graph\").</p> <code>'graph'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If start_node or end_node is not of type Node.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>graphorchestrator\\graph\\graph.py</code> <pre><code>def __init__(\n    self, start_node: Node, end_node: Node, name: Optional[str] = \"graph\"\n) -&gt; None:\n    \"\"\"\n    Initializes a Graph object.\n\n    Args:\n        start_node (Node): The starting node of the graph.\n        end_node (Node): The ending node of the graph.\n        name (Optional[str]): An optional name for the graph (default: \"graph\").\n\n    Raises:\n        TypeError: If start_node or end_node is not of type Node.\n\n    Returns:\n        None\n    \"\"\"\n    self.nodes: Dict[str, Node] = {}\n    if not isinstance(start_node, Node) or not isinstance(end_node, Node):\n        raise TypeError(\"start_node and end_node must be of type Node\")\n\n    self.concrete_edges: List[ConcreteEdge] = []\n    self.conditional_edges: List[ConditionalEdge] = []\n    self.start_node = start_node\n    self.end_node = end_node\n\n    GraphLogger.get().info(\n        **wrap_constants(\n            message=\"Graph initialized\",\n            **{\n                LC.EVENT_TYPE: \"graph\",\n                LC.ACTION: \"graph_initialized\",\n                LC.CUSTOM: {\n                    \"start_node_id\": start_node.node_id,\n                    \"end_node_id\": end_node.node_id,\n                },\n            }\n        )\n    )\n</code></pre>"},{"location":"reference/GraphBuilder/","title":"GraphBuilder","text":""},{"location":"reference/GraphBuilder/#graphorchestrator.graph.builder","title":"<code>graphorchestrator.graph.builder</code>","text":""},{"location":"reference/GraphBuilder/#graphorchestrator.graph.builder.GraphBuilder","title":"<code>GraphBuilder</code>","text":"<p>GraphBuilder is a utility class for constructing complex directed graphs.</p> <p>It supports adding various types of nodes (ProcessingNode, AggregatorNode) and edges (ConcreteEdge, ConditionalEdge) to define the flow and logic of a processing pipeline. It also handles error handling and logging.</p> Source code in <code>graphorchestrator\\graph\\builder.py</code> <pre><code>class GraphBuilder:\n    \"\"\"\n    GraphBuilder is a utility class for constructing complex directed graphs.\n\n    It supports adding various types of nodes (ProcessingNode, AggregatorNode) and\n    edges (ConcreteEdge, ConditionalEdge) to define the flow and logic of a\n    processing pipeline. It also handles error handling and logging.\n    \"\"\"\n\n    def __init__(self, name: Optional[str] = \"graph\"):\n        GraphLogger.get().info(\n            **wrap_constants(\n                message=\"GraphBuilder initialized\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"builder_init\",\n                    LC.CUSTOM: {\"graph_name\": name},\n                }\n            )\n        )\n\n        start_node = ProcessingNode(\"start\", passThrough)\n        end_node = ProcessingNode(\"end\", passThrough)\n        self.graph = Graph(start_node, end_node, name)\n        self.add_node(start_node)\n        self.add_node(end_node)\n\n    def add_node(self, node):\n        \"\"\"\n        Adds a node to the graph.\n\n        Args:\n            node: The node to be added.\n        Raises:\n             DuplicateNodeError: if there is already a node with the same id in the graph.\n        \"\"\"\n        log = GraphLogger.get()\n\n        log.debug(\n            **wrap_constants(\n                message=\"Attempting to add node to graph\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"add_node_attempt\",\n                    LC.NODE_ID: node.node_id,\n                }\n            )\n        )\n\n        if node.node_id in self.graph.nodes:\n            log.error(\n                **wrap_constants(\n                    message=\"Duplicate node detected\",\n                    **{\n                        LC.EVENT_TYPE: \"graph\",\n                        LC.ACTION: \"duplicate_node\",\n                        LC.NODE_ID: node.node_id,\n                    }\n                )\n            )\n            raise DuplicateNodeError(node.node_id)\n\n        self.graph.nodes[node.node_id] = node\n\n        log.info(\n            **wrap_constants(\n                message=\"Node successfully added to graph\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"node_added\",\n                    LC.NODE_ID: node.node_id,\n                    LC.NODE_TYPE: node.__class__.__name__,\n                }\n            )\n        )\n\n    def set_fallback_node(self, node_id: str, fallback_node_id: str):\n        \"\"\"\n        Sets a fallback node for a given node.\n\n        In case of failure of the node, the graph will execute the fallback node.\n\n        Args:\n            node_id: The ID of the node for which to set a fallback.\n            fallback_node_id: The ID of the fallback node.\n\n        Raises:\n            NodeNotFoundError: if the node or fallback node does not exist in the graph.\n        \"\"\"\n        log = GraphLogger.get()\n\n        log.debug(\n            **wrap_constants(\n                message=\"Attempting to set fallback node\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"set_fallback_attempt\",\n                    LC.NODE_ID: node_id,\n                    LC.FALLBACK_NODE: fallback_node_id,\n                }\n            )\n        )\n\n        if node_id not in self.graph.nodes:\n            log.error(\n                **wrap_constants(\n                    message=\"Primary node not found for fallback assignment\",\n                    **{\n                        LC.EVENT_TYPE: \"graph\",\n                        LC.ACTION: \"fallback_assignment_failed\",\n                        LC.NODE_ID: node_id,\n                        LC.FALLBACK_NODE: fallback_node_id,\n                        LC.CUSTOM: {\"reason\": \"node_id does not exist\"},\n                    }\n                )\n            )\n            raise NodeNotFoundError(node_id)\n\n        if fallback_node_id not in self.graph.nodes:\n            log.error(\n                **wrap_constants(\n                    message=\"Fallback node not found in graph\",\n                    **{\n                        LC.EVENT_TYPE: \"graph\",\n                        LC.ACTION: \"fallback_assignment_failed\",\n                        LC.NODE_ID: node_id,\n                        LC.FALLBACK_NODE: fallback_node_id,\n                        LC.CUSTOM: {\"reason\": \"fallback_node_id does not exist\"},\n                    }\n                )\n            )\n            raise NodeNotFoundError(fallback_node_id)\n\n        self.graph.nodes[node_id].set_fallback(fallback_node_id)\n\n        log.info(\n            **wrap_constants(\n                message=\"Fallback node set successfully\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"fallback_assigned\",\n                    LC.NODE_ID: node_id,\n                    LC.FALLBACK_NODE: fallback_node_id,\n                }\n            )\n        )\n\n    def set_node_retry_policy(self, node_id: str, retry_policy: RetryPolicy) -&gt; None:\n        \"\"\"\n        Sets a retry policy for a given node.\n\n        The node will retry upon failure as per the given policy.\n\n        Args:\n            node_id: The ID of the node for which to set the retry policy.\n            retry_policy: The retry policy to set.\n        Raises:\n            NodeNotFoundError: if the node does not exist in the graph.\n        \"\"\"\n        log = GraphLogger.get()\n\n        log.debug(\n            **wrap_constants(\n                message=\"Attempting to set retry policy for node\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"set_retry_policy_attempt\",\n                    LC.NODE_ID: node_id,\n                    LC.CUSTOM: {\n                        \"max_retries\": retry_policy.max_retries,\n                        \"delay\": retry_policy.delay,\n                        \"backoff\": retry_policy.backoff,\n                    },\n                }\n            )\n        )\n\n        if node_id not in self.graph.nodes:\n            log.error(\n                **wrap_constants(\n                    message=\"Cannot set retry policy \u2014 node not found\",\n                    **{\n                        LC.EVENT_TYPE: \"graph\",\n                        LC.ACTION: \"set_retry_policy_failed\",\n                        LC.NODE_ID: node_id,\n                        LC.CUSTOM: {\"reason\": \"node_id does not exist\"},\n                    }\n                )\n            )\n            raise NodeNotFoundError(node_id)\n\n        self.graph.nodes[node_id].set_retry_policy(retry_policy)\n\n        log.info(\n            **wrap_constants(\n                message=\"Retry policy set successfully\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"retry_policy_assigned\",\n                    LC.NODE_ID: node_id,\n                    LC.CUSTOM: {\n                        \"max_retries\": retry_policy.max_retries,\n                        \"delay\": retry_policy.delay,\n                        \"backoff\": retry_policy.backoff,\n                    },\n                }\n            )\n        )\n\n    def add_aggregator(self, aggregator: AggregatorNode):\n        \"\"\"\n        Adds an aggregator node to the graph.\n\n        Args:\n            aggregator: The aggregator node to add.\n        Raises:\n             DuplicateNodeError: if there is already a node with the same id in the graph.\n        \"\"\"\n        log = GraphLogger.get()\n\n        log.debug(\n            **wrap_constants(\n                message=\"Attempting to add aggregator node to graph\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"add_aggregator_attempt\",\n                    LC.NODE_ID: aggregator.node_id,\n                    LC.NODE_TYPE: \"AggregatorNode\",\n                }\n            )\n        )\n\n        if aggregator.node_id in self.graph.nodes:\n            log.error(\n                **wrap_constants(\n                    message=\"Duplicate aggregator node detected\",\n                    **{\n                        LC.EVENT_TYPE: \"graph\",\n                        LC.ACTION: \"duplicate_node\",\n                        LC.NODE_ID: aggregator.node_id,\n                        LC.NODE_TYPE: \"AggregatorNode\",\n                    }\n                )\n            )\n            raise DuplicateNodeError(aggregator.node_id)\n\n        self.graph.nodes[aggregator.node_id] = aggregator\n\n        log.info(\n            **wrap_constants(\n                message=\"Aggregator node registered in graph\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"aggregator_registered\",\n                    LC.NODE_ID: aggregator.node_id,\n                    LC.NODE_TYPE: \"AggregatorNode\",\n                }\n            )\n        )\n\n    def add_concrete_edge(self, source_id: str, sink_id: str):\n        \"\"\"\n        Adds a concrete edge between two nodes.\n\n        Args:\n            source_id: The ID of the source node.\n            sink_id: The ID of the sink node.\n\n        Raises:\n            NodeNotFoundError: if the source or sink node does not exist in the graph.\n            EdgeExistsError: if an edge already exists between the source and sink.\n        \"\"\"\n        log = GraphLogger.get()\n\n        log.debug(\n            **wrap_constants(\n                message=\"Attempting to add concrete edge\",\n                **{\n                    LC.EVENT_TYPE: \"edge\",\n                    LC.ACTION: \"add_concrete_edge_attempt\",\n                    LC.SOURCE_NODE: source_id,\n                    LC.SINK_NODE: sink_id,\n                    LC.EDGE_TYPE: \"concrete\",\n                }\n            )\n        )\n\n        if source_id not in self.graph.nodes:\n            log.error(\n                **wrap_constants(\n                    message=\"Concrete edge source node not found\",\n                    **{\n                        LC.EVENT_TYPE: \"edge\",\n                        LC.ACTION: \"add_concrete_edge_failed\",\n                        LC.SOURCE_NODE: source_id,\n                        LC.SINK_NODE: sink_id,\n                        LC.CUSTOM: {\"reason\": \"source_id not in graph\"},\n                    }\n                )\n            )\n            raise NodeNotFoundError(source_id)\n\n        if source_id == \"end\":\n            raise GraphConfigurationError(\"End cannot be the source of a concrete edge\")\n\n        if sink_id not in self.graph.nodes:\n            log.error(\n                **wrap_constants(\n                    message=\"Concrete edge sink node not found\",\n                    **{\n                        LC.EVENT_TYPE: \"edge\",\n                        LC.ACTION: \"add_concrete_edge_failed\",\n                        LC.SOURCE_NODE: source_id,\n                        LC.SINK_NODE: sink_id,\n                        LC.CUSTOM: {\"reason\": \"sink_id not in graph\"},\n                    }\n                )\n            )\n            raise NodeNotFoundError(sink_id)\n\n        if sink_id == \"start\":\n            raise GraphConfigurationError(\"Start cannot be a sink of concrete edge\")\n\n        source = self.graph.nodes[source_id]\n        sink = self.graph.nodes[sink_id]\n\n        for edge in self.graph.concrete_edges:\n            if edge.source == source and edge.sink == sink:\n                log.error(\n                    **wrap_constants(\n                        message=\"Duplicate concrete edge detected\",\n                        **{\n                            LC.EVENT_TYPE: \"edge\",\n                            LC.ACTION: \"duplicate_edge\",\n                            LC.SOURCE_NODE: source_id,\n                            LC.SINK_NODE: sink_id,\n                        }\n                    )\n                )\n                raise EdgeExistsError(source_id, sink_id)\n\n        for cond_edge in self.graph.conditional_edges:\n            if cond_edge.source == source and sink in cond_edge.sinks:\n                log.error(\n                    **wrap_constants(\n                        message=\"Edge conflicts with existing conditional edge\",\n                        **{\n                            LC.EVENT_TYPE: \"edge\",\n                            LC.ACTION: \"conflict_with_conditional_edge\",\n                            LC.SOURCE_NODE: source_id,\n                            LC.SINK_NODE: sink_id,\n                        }\n                    )\n                )\n                raise EdgeExistsError(source_id, sink_id)\n\n        edge = ConcreteEdge(source, sink)\n        self.graph.concrete_edges.append(edge)\n        source.outgoing_edges.append(edge)\n        sink.incoming_edges.append(edge)\n\n        log.info(\n            **wrap_constants(\n                message=\"Concrete edge successfully added\",\n                **{\n                    LC.EVENT_TYPE: \"edge\",\n                    LC.ACTION: \"concrete_edge_added\",\n                    LC.SOURCE_NODE: source_id,\n                    LC.SINK_NODE: sink_id,\n                    LC.EDGE_TYPE: \"concrete\",\n                }\n            )\n        )\n\n    def add_conditional_edge(\n        self, source_id: str, sink_ids: List[str], router: Callable[[State], str]\n    ):\n        \"\"\"\n        Adds a conditional edge between a source node and multiple sink nodes,\n        using a router function to determine the sink based on the state.\n\n        Args:\n            source_id: The ID of the source node.\n            sink_ids: A list of IDs of the possible sink nodes.\n            router: A function that takes a State object and returns the ID of the\n                    chosen sink node.\n\n        Raises:\n            NodeNotFoundError: if the source or sink node does not exist in the graph.\n        \"\"\"\n        log = GraphLogger.get()\n\n        log.debug(\n            **wrap_constants(\n                message=\"Attempting to add conditional edge\",\n                **{\n                    LC.EVENT_TYPE: \"edge\",\n                    LC.ACTION: \"add_conditional_edge_attempt\",\n                    LC.SOURCE_NODE: source_id,\n                    LC.SINK_NODE: sink_ids,\n                    LC.EDGE_TYPE: \"conditional\",\n                    LC.ROUTER_FUNC: router.__name__,\n                }\n            )\n        )\n\n        if source_id not in self.graph.nodes:\n            log.error(\n                **wrap_constants(\n                    message=\"Conditional edge source node not found\",\n                    **{\n                        LC.EVENT_TYPE: \"edge\",\n                        LC.ACTION: \"add_conditional_edge_failed\",\n                        LC.SOURCE_NODE: source_id,\n                        LC.SINK_NODE: sink_ids,\n                        LC.CUSTOM: {\"reason\": \"source_id not in graph\"},\n                    }\n                )\n            )\n            raise NodeNotFoundError(source_id)\n\n        if source_id == \"end\":\n            raise GraphConfigurationError(\n                \"End cannot be the source of a conditional edge\"\n            )\n\n        source = self.graph.nodes[source_id]\n        sinks = []\n\n        for sink_id in sink_ids:\n            if sink_id not in self.graph.nodes:\n                log.error(\n                    **wrap_constants(\n                        message=\"Conditional edge sink node not found\",\n                        **{\n                            LC.EVENT_TYPE: \"edge\",\n                            LC.ACTION: \"add_conditional_edge_failed\",\n                            LC.SOURCE_NODE: source_id,\n                            LC.SINK_NODE: sink_id,\n                            LC.CUSTOM: {\"reason\": \"sink_id not in graph\"},\n                        }\n                    )\n                )\n                raise NodeNotFoundError(sink_id)\n\n            if sink_id == \"start\":\n                raise GraphConfigurationError(\n                    \"Start cannot be a sink of conditional edge\"\n                )\n\n            sinks.append(self.graph.nodes[sink_id])\n\n        for edge in self.graph.concrete_edges:\n            if edge.source == source and edge.sink in sinks:\n                log.error(\n                    **wrap_constants(\n                        message=\"Conflict with existing concrete edge\",\n                        **{\n                            LC.EVENT_TYPE: \"edge\",\n                            LC.ACTION: \"conflict_with_concrete_edge\",\n                            LC.SOURCE_NODE: source_id,\n                            LC.SINK_NODE: edge.sink.node_id,\n                        }\n                    )\n                )\n                raise EdgeExistsError(source_id, edge.sink.node_id)\n\n        for cond_edge in self.graph.conditional_edges:\n            if cond_edge.source == source:\n                for s in sinks:\n                    if s in cond_edge.sinks:\n                        log.error(\n                            **wrap_constants(\n                                message=\"Duplicate conditional edge branch detected\",\n                                **{\n                                    LC.EVENT_TYPE: \"edge\",\n                                    LC.ACTION: \"duplicate_conditional_branch\",\n                                    LC.SOURCE_NODE: source_id,\n                                    LC.SINK_NODE: s.node_id,\n                                }\n                            )\n                        )\n                        raise EdgeExistsError(source_id, s.node_id)\n\n        edge = ConditionalEdge(source, sinks, router)\n        self.graph.conditional_edges.append(edge)\n        source.outgoing_edges.append(edge)\n        for sink in sinks:\n            sink.incoming_edges.append(edge)\n\n        log.info(\n            **wrap_constants(\n                message=\"Conditional edge successfully added\",\n                **{\n                    LC.EVENT_TYPE: \"edge\",\n                    LC.ACTION: \"conditional_edge_added\",\n                    LC.SOURCE_NODE: source_id,\n                    LC.SINK_NODE: [s.node_id for s in sinks],\n                    LC.EDGE_TYPE: \"conditional\",\n                    LC.ROUTER_FUNC: router.__name__,\n                }\n            )\n        )\n\n    def build_graph(self) -&gt; Graph:\n        \"\"\"\n        Builds and validates the graph.\n\n        Performs a validation of the graph prior to build.\n\n        Returns:\n            The constructed Graph object.\n        Raises:\n            GraphConfigurationError: if the configuration of the graph is not valid.\n        \"\"\"\n        log = GraphLogger.get()\n\n        log.debug(\n            **wrap_constants(\n                message=\"Validating graph before build\",\n                **{LC.EVENT_TYPE: \"graph\", LC.ACTION: \"build_graph_validation_start\"}\n            )\n        )\n\n        start_node = self.graph.start_node\n\n        if any(isinstance(e, ConditionalEdge) for e in start_node.outgoing_edges):\n            log.error(\n                **wrap_constants(\n                    message=\"Start node has a conditional edge \u2014 invalid graph\",\n                    **{\n                        LC.EVENT_TYPE: \"graph\",\n                        LC.ACTION: \"build_graph_failed\",\n                        LC.CUSTOM: {\"reason\": \"start node has conditional edge\"},\n                    }\n                )\n            )\n            raise GraphConfigurationError(\"Start node cannot have a conditional edge\")\n\n        if not any(isinstance(e, ConcreteEdge) for e in start_node.outgoing_edges):\n            log.error(\n                **wrap_constants(\n                    message=\"Start node missing concrete edge \u2014 invalid graph\",\n                    **{\n                        LC.EVENT_TYPE: \"graph\",\n                        LC.ACTION: \"build_graph_failed\",\n                        LC.CUSTOM: {\n                            \"reason\": \"start node must have at least one concrete edge\"\n                        },\n                    }\n                )\n            )\n            raise GraphConfigurationError(\n                \"Start node must have at least one outgoing concrete edge\"\n            )\n\n        if not self.graph.end_node.incoming_edges:\n            log.error(\n                **wrap_constants(\n                    message=\"End node has no incoming edges \u2014 invalid graph\",\n                    **{\n                        LC.EVENT_TYPE: \"graph\",\n                        LC.ACTION: \"build_graph_failed\",\n                        LC.CUSTOM: {\n                            \"reason\": \"end node must have at least one incoming edge\"\n                        },\n                    }\n                )\n            )\n            raise GraphConfigurationError(\n                \"End node must have at least one incoming edge\"\n            )\n\n        log.info(\n            **wrap_constants(\n                message=\"Graph successfully built\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"graph_built\",\n                    LC.CUSTOM: {\n                        \"node_count\": len(self.graph.nodes),\n                        \"concrete_edges\": len(self.graph.concrete_edges),\n                        \"conditional_edges\": len(self.graph.conditional_edges),\n                    },\n                }\n            )\n        )\n\n        return self.graph\n</code></pre>"},{"location":"reference/GraphBuilder/#graphorchestrator.graph.builder.GraphBuilder.add_aggregator","title":"<code>add_aggregator(aggregator)</code>","text":"<p>Adds an aggregator node to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>aggregator</code> <code>AggregatorNode</code> <p>The aggregator node to add.</p> required <p>Raises:      DuplicateNodeError: if there is already a node with the same id in the graph.</p> Source code in <code>graphorchestrator\\graph\\builder.py</code> <pre><code>def add_aggregator(self, aggregator: AggregatorNode):\n    \"\"\"\n    Adds an aggregator node to the graph.\n\n    Args:\n        aggregator: The aggregator node to add.\n    Raises:\n         DuplicateNodeError: if there is already a node with the same id in the graph.\n    \"\"\"\n    log = GraphLogger.get()\n\n    log.debug(\n        **wrap_constants(\n            message=\"Attempting to add aggregator node to graph\",\n            **{\n                LC.EVENT_TYPE: \"graph\",\n                LC.ACTION: \"add_aggregator_attempt\",\n                LC.NODE_ID: aggregator.node_id,\n                LC.NODE_TYPE: \"AggregatorNode\",\n            }\n        )\n    )\n\n    if aggregator.node_id in self.graph.nodes:\n        log.error(\n            **wrap_constants(\n                message=\"Duplicate aggregator node detected\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"duplicate_node\",\n                    LC.NODE_ID: aggregator.node_id,\n                    LC.NODE_TYPE: \"AggregatorNode\",\n                }\n            )\n        )\n        raise DuplicateNodeError(aggregator.node_id)\n\n    self.graph.nodes[aggregator.node_id] = aggregator\n\n    log.info(\n        **wrap_constants(\n            message=\"Aggregator node registered in graph\",\n            **{\n                LC.EVENT_TYPE: \"graph\",\n                LC.ACTION: \"aggregator_registered\",\n                LC.NODE_ID: aggregator.node_id,\n                LC.NODE_TYPE: \"AggregatorNode\",\n            }\n        )\n    )\n</code></pre>"},{"location":"reference/GraphBuilder/#graphorchestrator.graph.builder.GraphBuilder.add_concrete_edge","title":"<code>add_concrete_edge(source_id, sink_id)</code>","text":"<p>Adds a concrete edge between two nodes.</p> <p>Parameters:</p> Name Type Description Default <code>source_id</code> <code>str</code> <p>The ID of the source node.</p> required <code>sink_id</code> <code>str</code> <p>The ID of the sink node.</p> required <p>Raises:</p> Type Description <code>NodeNotFoundError</code> <p>if the source or sink node does not exist in the graph.</p> <code>EdgeExistsError</code> <p>if an edge already exists between the source and sink.</p> Source code in <code>graphorchestrator\\graph\\builder.py</code> <pre><code>def add_concrete_edge(self, source_id: str, sink_id: str):\n    \"\"\"\n    Adds a concrete edge between two nodes.\n\n    Args:\n        source_id: The ID of the source node.\n        sink_id: The ID of the sink node.\n\n    Raises:\n        NodeNotFoundError: if the source or sink node does not exist in the graph.\n        EdgeExistsError: if an edge already exists between the source and sink.\n    \"\"\"\n    log = GraphLogger.get()\n\n    log.debug(\n        **wrap_constants(\n            message=\"Attempting to add concrete edge\",\n            **{\n                LC.EVENT_TYPE: \"edge\",\n                LC.ACTION: \"add_concrete_edge_attempt\",\n                LC.SOURCE_NODE: source_id,\n                LC.SINK_NODE: sink_id,\n                LC.EDGE_TYPE: \"concrete\",\n            }\n        )\n    )\n\n    if source_id not in self.graph.nodes:\n        log.error(\n            **wrap_constants(\n                message=\"Concrete edge source node not found\",\n                **{\n                    LC.EVENT_TYPE: \"edge\",\n                    LC.ACTION: \"add_concrete_edge_failed\",\n                    LC.SOURCE_NODE: source_id,\n                    LC.SINK_NODE: sink_id,\n                    LC.CUSTOM: {\"reason\": \"source_id not in graph\"},\n                }\n            )\n        )\n        raise NodeNotFoundError(source_id)\n\n    if source_id == \"end\":\n        raise GraphConfigurationError(\"End cannot be the source of a concrete edge\")\n\n    if sink_id not in self.graph.nodes:\n        log.error(\n            **wrap_constants(\n                message=\"Concrete edge sink node not found\",\n                **{\n                    LC.EVENT_TYPE: \"edge\",\n                    LC.ACTION: \"add_concrete_edge_failed\",\n                    LC.SOURCE_NODE: source_id,\n                    LC.SINK_NODE: sink_id,\n                    LC.CUSTOM: {\"reason\": \"sink_id not in graph\"},\n                }\n            )\n        )\n        raise NodeNotFoundError(sink_id)\n\n    if sink_id == \"start\":\n        raise GraphConfigurationError(\"Start cannot be a sink of concrete edge\")\n\n    source = self.graph.nodes[source_id]\n    sink = self.graph.nodes[sink_id]\n\n    for edge in self.graph.concrete_edges:\n        if edge.source == source and edge.sink == sink:\n            log.error(\n                **wrap_constants(\n                    message=\"Duplicate concrete edge detected\",\n                    **{\n                        LC.EVENT_TYPE: \"edge\",\n                        LC.ACTION: \"duplicate_edge\",\n                        LC.SOURCE_NODE: source_id,\n                        LC.SINK_NODE: sink_id,\n                    }\n                )\n            )\n            raise EdgeExistsError(source_id, sink_id)\n\n    for cond_edge in self.graph.conditional_edges:\n        if cond_edge.source == source and sink in cond_edge.sinks:\n            log.error(\n                **wrap_constants(\n                    message=\"Edge conflicts with existing conditional edge\",\n                    **{\n                        LC.EVENT_TYPE: \"edge\",\n                        LC.ACTION: \"conflict_with_conditional_edge\",\n                        LC.SOURCE_NODE: source_id,\n                        LC.SINK_NODE: sink_id,\n                    }\n                )\n            )\n            raise EdgeExistsError(source_id, sink_id)\n\n    edge = ConcreteEdge(source, sink)\n    self.graph.concrete_edges.append(edge)\n    source.outgoing_edges.append(edge)\n    sink.incoming_edges.append(edge)\n\n    log.info(\n        **wrap_constants(\n            message=\"Concrete edge successfully added\",\n            **{\n                LC.EVENT_TYPE: \"edge\",\n                LC.ACTION: \"concrete_edge_added\",\n                LC.SOURCE_NODE: source_id,\n                LC.SINK_NODE: sink_id,\n                LC.EDGE_TYPE: \"concrete\",\n            }\n        )\n    )\n</code></pre>"},{"location":"reference/GraphBuilder/#graphorchestrator.graph.builder.GraphBuilder.add_conditional_edge","title":"<code>add_conditional_edge(source_id, sink_ids, router)</code>","text":"<p>Adds a conditional edge between a source node and multiple sink nodes, using a router function to determine the sink based on the state.</p> <p>Parameters:</p> Name Type Description Default <code>source_id</code> <code>str</code> <p>The ID of the source node.</p> required <code>sink_ids</code> <code>List[str]</code> <p>A list of IDs of the possible sink nodes.</p> required <code>router</code> <code>Callable[[State], str]</code> <p>A function that takes a State object and returns the ID of the     chosen sink node.</p> required <p>Raises:</p> Type Description <code>NodeNotFoundError</code> <p>if the source or sink node does not exist in the graph.</p> Source code in <code>graphorchestrator\\graph\\builder.py</code> <pre><code>def add_conditional_edge(\n    self, source_id: str, sink_ids: List[str], router: Callable[[State], str]\n):\n    \"\"\"\n    Adds a conditional edge between a source node and multiple sink nodes,\n    using a router function to determine the sink based on the state.\n\n    Args:\n        source_id: The ID of the source node.\n        sink_ids: A list of IDs of the possible sink nodes.\n        router: A function that takes a State object and returns the ID of the\n                chosen sink node.\n\n    Raises:\n        NodeNotFoundError: if the source or sink node does not exist in the graph.\n    \"\"\"\n    log = GraphLogger.get()\n\n    log.debug(\n        **wrap_constants(\n            message=\"Attempting to add conditional edge\",\n            **{\n                LC.EVENT_TYPE: \"edge\",\n                LC.ACTION: \"add_conditional_edge_attempt\",\n                LC.SOURCE_NODE: source_id,\n                LC.SINK_NODE: sink_ids,\n                LC.EDGE_TYPE: \"conditional\",\n                LC.ROUTER_FUNC: router.__name__,\n            }\n        )\n    )\n\n    if source_id not in self.graph.nodes:\n        log.error(\n            **wrap_constants(\n                message=\"Conditional edge source node not found\",\n                **{\n                    LC.EVENT_TYPE: \"edge\",\n                    LC.ACTION: \"add_conditional_edge_failed\",\n                    LC.SOURCE_NODE: source_id,\n                    LC.SINK_NODE: sink_ids,\n                    LC.CUSTOM: {\"reason\": \"source_id not in graph\"},\n                }\n            )\n        )\n        raise NodeNotFoundError(source_id)\n\n    if source_id == \"end\":\n        raise GraphConfigurationError(\n            \"End cannot be the source of a conditional edge\"\n        )\n\n    source = self.graph.nodes[source_id]\n    sinks = []\n\n    for sink_id in sink_ids:\n        if sink_id not in self.graph.nodes:\n            log.error(\n                **wrap_constants(\n                    message=\"Conditional edge sink node not found\",\n                    **{\n                        LC.EVENT_TYPE: \"edge\",\n                        LC.ACTION: \"add_conditional_edge_failed\",\n                        LC.SOURCE_NODE: source_id,\n                        LC.SINK_NODE: sink_id,\n                        LC.CUSTOM: {\"reason\": \"sink_id not in graph\"},\n                    }\n                )\n            )\n            raise NodeNotFoundError(sink_id)\n\n        if sink_id == \"start\":\n            raise GraphConfigurationError(\n                \"Start cannot be a sink of conditional edge\"\n            )\n\n        sinks.append(self.graph.nodes[sink_id])\n\n    for edge in self.graph.concrete_edges:\n        if edge.source == source and edge.sink in sinks:\n            log.error(\n                **wrap_constants(\n                    message=\"Conflict with existing concrete edge\",\n                    **{\n                        LC.EVENT_TYPE: \"edge\",\n                        LC.ACTION: \"conflict_with_concrete_edge\",\n                        LC.SOURCE_NODE: source_id,\n                        LC.SINK_NODE: edge.sink.node_id,\n                    }\n                )\n            )\n            raise EdgeExistsError(source_id, edge.sink.node_id)\n\n    for cond_edge in self.graph.conditional_edges:\n        if cond_edge.source == source:\n            for s in sinks:\n                if s in cond_edge.sinks:\n                    log.error(\n                        **wrap_constants(\n                            message=\"Duplicate conditional edge branch detected\",\n                            **{\n                                LC.EVENT_TYPE: \"edge\",\n                                LC.ACTION: \"duplicate_conditional_branch\",\n                                LC.SOURCE_NODE: source_id,\n                                LC.SINK_NODE: s.node_id,\n                            }\n                        )\n                    )\n                    raise EdgeExistsError(source_id, s.node_id)\n\n    edge = ConditionalEdge(source, sinks, router)\n    self.graph.conditional_edges.append(edge)\n    source.outgoing_edges.append(edge)\n    for sink in sinks:\n        sink.incoming_edges.append(edge)\n\n    log.info(\n        **wrap_constants(\n            message=\"Conditional edge successfully added\",\n            **{\n                LC.EVENT_TYPE: \"edge\",\n                LC.ACTION: \"conditional_edge_added\",\n                LC.SOURCE_NODE: source_id,\n                LC.SINK_NODE: [s.node_id for s in sinks],\n                LC.EDGE_TYPE: \"conditional\",\n                LC.ROUTER_FUNC: router.__name__,\n            }\n        )\n    )\n</code></pre>"},{"location":"reference/GraphBuilder/#graphorchestrator.graph.builder.GraphBuilder.add_node","title":"<code>add_node(node)</code>","text":"<p>Adds a node to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <p>The node to be added.</p> required <p>Raises:      DuplicateNodeError: if there is already a node with the same id in the graph.</p> Source code in <code>graphorchestrator\\graph\\builder.py</code> <pre><code>def add_node(self, node):\n    \"\"\"\n    Adds a node to the graph.\n\n    Args:\n        node: The node to be added.\n    Raises:\n         DuplicateNodeError: if there is already a node with the same id in the graph.\n    \"\"\"\n    log = GraphLogger.get()\n\n    log.debug(\n        **wrap_constants(\n            message=\"Attempting to add node to graph\",\n            **{\n                LC.EVENT_TYPE: \"graph\",\n                LC.ACTION: \"add_node_attempt\",\n                LC.NODE_ID: node.node_id,\n            }\n        )\n    )\n\n    if node.node_id in self.graph.nodes:\n        log.error(\n            **wrap_constants(\n                message=\"Duplicate node detected\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"duplicate_node\",\n                    LC.NODE_ID: node.node_id,\n                }\n            )\n        )\n        raise DuplicateNodeError(node.node_id)\n\n    self.graph.nodes[node.node_id] = node\n\n    log.info(\n        **wrap_constants(\n            message=\"Node successfully added to graph\",\n            **{\n                LC.EVENT_TYPE: \"graph\",\n                LC.ACTION: \"node_added\",\n                LC.NODE_ID: node.node_id,\n                LC.NODE_TYPE: node.__class__.__name__,\n            }\n        )\n    )\n</code></pre>"},{"location":"reference/GraphBuilder/#graphorchestrator.graph.builder.GraphBuilder.build_graph","title":"<code>build_graph()</code>","text":"<p>Builds and validates the graph.</p> <p>Performs a validation of the graph prior to build.</p> <p>Returns:</p> Type Description <code>Graph</code> <p>The constructed Graph object.</p> <p>Raises:     GraphConfigurationError: if the configuration of the graph is not valid.</p> Source code in <code>graphorchestrator\\graph\\builder.py</code> <pre><code>def build_graph(self) -&gt; Graph:\n    \"\"\"\n    Builds and validates the graph.\n\n    Performs a validation of the graph prior to build.\n\n    Returns:\n        The constructed Graph object.\n    Raises:\n        GraphConfigurationError: if the configuration of the graph is not valid.\n    \"\"\"\n    log = GraphLogger.get()\n\n    log.debug(\n        **wrap_constants(\n            message=\"Validating graph before build\",\n            **{LC.EVENT_TYPE: \"graph\", LC.ACTION: \"build_graph_validation_start\"}\n        )\n    )\n\n    start_node = self.graph.start_node\n\n    if any(isinstance(e, ConditionalEdge) for e in start_node.outgoing_edges):\n        log.error(\n            **wrap_constants(\n                message=\"Start node has a conditional edge \u2014 invalid graph\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"build_graph_failed\",\n                    LC.CUSTOM: {\"reason\": \"start node has conditional edge\"},\n                }\n            )\n        )\n        raise GraphConfigurationError(\"Start node cannot have a conditional edge\")\n\n    if not any(isinstance(e, ConcreteEdge) for e in start_node.outgoing_edges):\n        log.error(\n            **wrap_constants(\n                message=\"Start node missing concrete edge \u2014 invalid graph\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"build_graph_failed\",\n                    LC.CUSTOM: {\n                        \"reason\": \"start node must have at least one concrete edge\"\n                    },\n                }\n            )\n        )\n        raise GraphConfigurationError(\n            \"Start node must have at least one outgoing concrete edge\"\n        )\n\n    if not self.graph.end_node.incoming_edges:\n        log.error(\n            **wrap_constants(\n                message=\"End node has no incoming edges \u2014 invalid graph\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"build_graph_failed\",\n                    LC.CUSTOM: {\n                        \"reason\": \"end node must have at least one incoming edge\"\n                    },\n                }\n            )\n        )\n        raise GraphConfigurationError(\n            \"End node must have at least one incoming edge\"\n        )\n\n    log.info(\n        **wrap_constants(\n            message=\"Graph successfully built\",\n            **{\n                LC.EVENT_TYPE: \"graph\",\n                LC.ACTION: \"graph_built\",\n                LC.CUSTOM: {\n                    \"node_count\": len(self.graph.nodes),\n                    \"concrete_edges\": len(self.graph.concrete_edges),\n                    \"conditional_edges\": len(self.graph.conditional_edges),\n                },\n            }\n        )\n    )\n\n    return self.graph\n</code></pre>"},{"location":"reference/GraphBuilder/#graphorchestrator.graph.builder.GraphBuilder.set_fallback_node","title":"<code>set_fallback_node(node_id, fallback_node_id)</code>","text":"<p>Sets a fallback node for a given node.</p> <p>In case of failure of the node, the graph will execute the fallback node.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node for which to set a fallback.</p> required <code>fallback_node_id</code> <code>str</code> <p>The ID of the fallback node.</p> required <p>Raises:</p> Type Description <code>NodeNotFoundError</code> <p>if the node or fallback node does not exist in the graph.</p> Source code in <code>graphorchestrator\\graph\\builder.py</code> <pre><code>def set_fallback_node(self, node_id: str, fallback_node_id: str):\n    \"\"\"\n    Sets a fallback node for a given node.\n\n    In case of failure of the node, the graph will execute the fallback node.\n\n    Args:\n        node_id: The ID of the node for which to set a fallback.\n        fallback_node_id: The ID of the fallback node.\n\n    Raises:\n        NodeNotFoundError: if the node or fallback node does not exist in the graph.\n    \"\"\"\n    log = GraphLogger.get()\n\n    log.debug(\n        **wrap_constants(\n            message=\"Attempting to set fallback node\",\n            **{\n                LC.EVENT_TYPE: \"graph\",\n                LC.ACTION: \"set_fallback_attempt\",\n                LC.NODE_ID: node_id,\n                LC.FALLBACK_NODE: fallback_node_id,\n            }\n        )\n    )\n\n    if node_id not in self.graph.nodes:\n        log.error(\n            **wrap_constants(\n                message=\"Primary node not found for fallback assignment\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"fallback_assignment_failed\",\n                    LC.NODE_ID: node_id,\n                    LC.FALLBACK_NODE: fallback_node_id,\n                    LC.CUSTOM: {\"reason\": \"node_id does not exist\"},\n                }\n            )\n        )\n        raise NodeNotFoundError(node_id)\n\n    if fallback_node_id not in self.graph.nodes:\n        log.error(\n            **wrap_constants(\n                message=\"Fallback node not found in graph\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"fallback_assignment_failed\",\n                    LC.NODE_ID: node_id,\n                    LC.FALLBACK_NODE: fallback_node_id,\n                    LC.CUSTOM: {\"reason\": \"fallback_node_id does not exist\"},\n                }\n            )\n        )\n        raise NodeNotFoundError(fallback_node_id)\n\n    self.graph.nodes[node_id].set_fallback(fallback_node_id)\n\n    log.info(\n        **wrap_constants(\n            message=\"Fallback node set successfully\",\n            **{\n                LC.EVENT_TYPE: \"graph\",\n                LC.ACTION: \"fallback_assigned\",\n                LC.NODE_ID: node_id,\n                LC.FALLBACK_NODE: fallback_node_id,\n            }\n        )\n    )\n</code></pre>"},{"location":"reference/GraphBuilder/#graphorchestrator.graph.builder.GraphBuilder.set_node_retry_policy","title":"<code>set_node_retry_policy(node_id, retry_policy)</code>","text":"<p>Sets a retry policy for a given node.</p> <p>The node will retry upon failure as per the given policy.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node for which to set the retry policy.</p> required <code>retry_policy</code> <code>RetryPolicy</code> <p>The retry policy to set.</p> required <p>Raises:     NodeNotFoundError: if the node does not exist in the graph.</p> Source code in <code>graphorchestrator\\graph\\builder.py</code> <pre><code>def set_node_retry_policy(self, node_id: str, retry_policy: RetryPolicy) -&gt; None:\n    \"\"\"\n    Sets a retry policy for a given node.\n\n    The node will retry upon failure as per the given policy.\n\n    Args:\n        node_id: The ID of the node for which to set the retry policy.\n        retry_policy: The retry policy to set.\n    Raises:\n        NodeNotFoundError: if the node does not exist in the graph.\n    \"\"\"\n    log = GraphLogger.get()\n\n    log.debug(\n        **wrap_constants(\n            message=\"Attempting to set retry policy for node\",\n            **{\n                LC.EVENT_TYPE: \"graph\",\n                LC.ACTION: \"set_retry_policy_attempt\",\n                LC.NODE_ID: node_id,\n                LC.CUSTOM: {\n                    \"max_retries\": retry_policy.max_retries,\n                    \"delay\": retry_policy.delay,\n                    \"backoff\": retry_policy.backoff,\n                },\n            }\n        )\n    )\n\n    if node_id not in self.graph.nodes:\n        log.error(\n            **wrap_constants(\n                message=\"Cannot set retry policy \u2014 node not found\",\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"set_retry_policy_failed\",\n                    LC.NODE_ID: node_id,\n                    LC.CUSTOM: {\"reason\": \"node_id does not exist\"},\n                }\n            )\n        )\n        raise NodeNotFoundError(node_id)\n\n    self.graph.nodes[node_id].set_retry_policy(retry_policy)\n\n    log.info(\n        **wrap_constants(\n            message=\"Retry policy set successfully\",\n            **{\n                LC.EVENT_TYPE: \"graph\",\n                LC.ACTION: \"retry_policy_assigned\",\n                LC.NODE_ID: node_id,\n                LC.CUSTOM: {\n                    \"max_retries\": retry_policy.max_retries,\n                    \"delay\": retry_policy.delay,\n                    \"backoff\": retry_policy.backoff,\n                },\n            }\n        )\n    )\n</code></pre>"},{"location":"reference/GraphExecutor/","title":"GraphExecutor","text":""},{"location":"reference/GraphExecutor/#graphorchestrator.graph.executor","title":"<code>graphorchestrator.graph.executor</code>","text":""},{"location":"reference/GraphExecutor/#graphorchestrator.graph.executor.GraphExecutor","title":"<code>GraphExecutor</code>","text":"<p>GraphExecutor is responsible for executing a graph by iterating over its nodes in supersteps. It manages the execution flow, retry policies, checkpointing, and fallback mechanisms.</p> <p>Attributes:</p> Name Type Description <code>graph</code> <p>The graph to execute.</p> <code>initial_state</code> <p>The initial state of the graph execution.</p> <code>max_workers</code> <p>The maximum number of concurrent node executions.</p> <code>retry_policy</code> <p>The retry policy for node executions.</p> <code>checkpoint_path</code> <p>The path to save/load checkpoints.</p> <code>checkpoint_every</code> <p>The frequency (in supersteps) to save checkpoints.</p> <code>allow_fallback_from_checkpoint</code> <p>Whether to fallback to the last checkpoint in case of timeout.</p> <code>active_states</code> <code>Dict[str, List[State]]</code> <p>The states of the active nodes in the current superstep.</p> <code>final_state</code> <p>The final state of the execution, when the graph is fully executed.</p> Source code in <code>graphorchestrator\\graph\\executor.py</code> <pre><code>class GraphExecutor:\n    \"\"\"\n    GraphExecutor is responsible for executing a graph by iterating over its nodes in supersteps.\n    It manages the execution flow, retry policies, checkpointing, and fallback mechanisms.\n\n    Attributes:\n        graph: The graph to execute.\n        initial_state: The initial state of the graph execution.\n        max_workers: The maximum number of concurrent node executions.\n        retry_policy: The retry policy for node executions.\n        checkpoint_path: The path to save/load checkpoints.\n        checkpoint_every: The frequency (in supersteps) to save checkpoints.\n        allow_fallback_from_checkpoint: Whether to fallback to the last checkpoint in case of timeout.\n        active_states: The states of the active nodes in the current superstep.\n        final_state: The final state of the execution, when the graph is fully executed.\n    \"\"\"\n\n    def __init__(\n        self,\n        graph,\n        initial_state,\n        max_workers: int = 4,\n        retry_policy: Optional[RetryPolicy] = None,\n        checkpoint_path: Optional[str] = None,\n        checkpoint_every: Optional[int] = None,\n        allow_fallback_from_checkpoint: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the GraphExecutor with the given parameters.\n\n        Args:\n            graph: The graph to execute.\n            initial_state: The initial state of the graph execution.\n            max_workers: The maximum number of concurrent node executions. Defaults to 4.\n            retry_policy: The retry policy for node executions. Defaults to no retries.\n            checkpoint_path: The path to save/load checkpoints. Defaults to None.\n            checkpoint_every: The frequency (in supersteps) to save checkpoints. Defaults to None.\n            allow_fallback_from_checkpoint: Whether to fallback to the last checkpoint in case of timeout. Defaults to False.\n        \"\"\"\n        LogContext.set(\n            {\n                LC.RUN_ID: str(uuid.uuid4()),\n                LC.GRAPH_NAME: getattr(graph, \"name\", None),\n                LC.USER_ID: getpass.getuser(),\n                LC.HOSTNAME: socket.gethostname(),\n            }\n        )\n        log = GraphLogger.get()\n        log.info(\n            **wrap_constants(\n                message=\"GraphExecutor initialized\",\n                **{\n                    LC.EVENT_TYPE: \"executor\",\n                    LC.ACTION: \"executor_init\",\n                    LC.CUSTOM: {\n                        \"max_workers\": max_workers,\n                        \"checkpoint_enabled\": bool(checkpoint_path),\n                        \"checkpoint_every\": checkpoint_every,\n                        \"allow_fallback_from_checkpoint\": allow_fallback_from_checkpoint,\n                        \"retry_policy\": {\n                            \"max_retries\": (\n                                retry_policy.max_retries if retry_policy else 0\n                            ),\n                            \"delay\": retry_policy.delay if retry_policy else 0,\n                            \"backoff\": retry_policy.backoff if retry_policy else 1,\n                        },\n                    },\n                },\n            )\n        )\n\n        self.graph = graph\n        self.initial_state = initial_state\n        self.max_workers = max_workers\n        self.active_states: Dict[str, List[State]] = defaultdict(list)\n        self.active_states[graph.start_node.node_id].append(initial_state)\n        self.retry_policy = (\n            retry_policy if retry_policy else RetryPolicy(max_retries=0, delay=0)\n        )\n        self.semaphore = asyncio.Semaphore(self.max_workers)\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.superstep = 0\n        self.final_state = None\n        self.allow_fallback_from_checkpoint = allow_fallback_from_checkpoint\n        self.already_retried_from_checkpoint = False\n\n        if self.allow_fallback_from_checkpoint and not self.checkpoint_path:\n            log.error(\n                **wrap_constants(\n                    message=\"Checkpoint fallback enabled without path\",\n                    **{\n                        LC.EVENT_TYPE: \"executor\",\n                        LC.ACTION: \"executor_init_failed\",\n                        LC.CUSTOM: {\n                            \"reason\": \"allow_fallback_from_checkpoint=True but checkpoint_path=None\"\n                        },\n                    },\n                )\n            )\n            raise GraphExecutionError(\n                node_id=\"GraphExecutor\",\n                message=\"Fallback from checkpoint is enabled, but no checkpoint_path is provided.\",\n            )\n\n    def to_checkpoint(self) -&gt; CheckpointData:\n        \"\"\"\n        Creates a CheckpointData object representing the current state of the graph execution.\n\n        Returns:\n            A CheckpointData object.\n        \"\"\"\n        log = GraphLogger.get()\n\n        log.info(\n            **wrap_constants(\n                message=\"Serializing current graph state into checkpoint\",\n                **{\n                    LC.EVENT_TYPE: \"executor\",\n                    LC.ACTION: \"create_checkpoint\",\n                    LC.SUPERSTEP: self.superstep,\n                    LC.CUSTOM: {\n                        \"active_node_ids\": list(self.active_states.keys()),\n                        \"final_state_message_count\": (\n                            len(self.final_state.messages) if self.final_state else None\n                        ),\n                        \"max_workers\": self.max_workers,\n                        \"retry_policy\": {\n                            \"max_retries\": self.retry_policy.max_retries,\n                            \"delay\": self.retry_policy.delay,\n                            \"backoff\": self.retry_policy.backoff,\n                        },\n                    },\n                },\n            )\n        )\n\n        return CheckpointData(\n            graph=self.graph,\n            initial_state=self.initial_state,\n            active_states=self.active_states,\n            superstep=self.superstep,\n            final_state=self.final_state,\n            retry_policy=self.retry_policy,\n            max_workers=self.max_workers,\n        )\n\n    @classmethod\n    def from_checkpoint(\n        cls,\n        chkpt: CheckpointData,\n        checkpoint_path: Optional[str] = None,\n        checkpoint_every: Optional[int] = None,\n    ):\n        \"\"\"\n        Creates a GraphExecutor object from a CheckpointData object.\n\n        Args:\n            chkpt: The CheckpointData object to restore from.\n            checkpoint_path: The path to save/load checkpoints. Defaults to None.\n            checkpoint_every: The frequency (in supersteps) to save checkpoints. Defaults to None.\n\n        Returns:\n            A GraphExecutor object restored from the checkpoint.\n\n        \"\"\"\n        log = GraphLogger.get()\n\n        log.info(\n            **wrap_constants(\n                message=\"Restoring executor from checkpoint\",\n                **{\n                    LC.EVENT_TYPE: \"executor\",\n                    LC.ACTION: \"restore_from_checkpoint\",\n                    LC.SUPERSTEP: chkpt.superstep,\n                    LC.CUSTOM: {\n                        \"active_node_ids\": list(chkpt.active_states.keys()),\n                        \"final_state_message_count\": (\n                            len(chkpt.final_state.messages)\n                            if chkpt.final_state\n                            else None\n                        ),\n                        \"max_workers\": chkpt.max_workers,\n                        \"checkpoint_path\": checkpoint_path,\n                        \"checkpoint_every\": checkpoint_every,\n                    },\n                },\n            )\n        )\n\n        executor = cls(\n            graph=chkpt.graph,\n            initial_state=chkpt.initial_state,\n            max_workers=chkpt.max_workers,\n            retry_policy=chkpt.retry_policy,\n            checkpoint_path=checkpoint_path,\n            checkpoint_every=checkpoint_every,\n        )\n        executor.active_states = chkpt.active_states\n        executor.superstep = chkpt.superstep\n        executor.final_state = chkpt.final_state\n        return executor\n\n    async def _execute_node_with_retry_async(\n        self, node, input_data, retry_policy\n    ) -&gt; None:\n        \"\"\"\n        Executes a node with the given input data, applying the retry policy.\n        This method is async and uses a semaphore to limit concurrency.\n\n        Args:\n            node: The node to execute.\n            input_data: The input data for the node.\n            retry_policy: The retry policy to apply.\n\n        Raises:\n            Exception: If the node execution fails after all retries.\n        \"\"\"\n        log = GraphLogger.get()\n\n        retry_policy = (\n            node.retry_policy if node.retry_policy is not None else retry_policy\n        )\n        attempt = 0\n        delay = retry_policy.delay\n\n        while attempt &lt;= retry_policy.max_retries:\n            async with self.semaphore:\n                try:\n                    log.info(\n                        **wrap_constants(\n                            message=\"Executing node with retry\",\n                            **{\n                                LC.EVENT_TYPE: \"node\",\n                                LC.ACTION: \"node_execution_attempt\",\n                                LC.NODE_ID: node.node_id,\n                                LC.RETRY_COUNT: attempt,\n                                LC.MAX_RETRIES: retry_policy.max_retries,\n                                LC.RETRY_DELAY: delay,\n                            },\n                        )\n                    )\n\n                    return await node.execute(input_data)\n\n                except Exception as e:\n                    if attempt == retry_policy.max_retries:\n                        log.error(\n                            **wrap_constants(\n                                message=\"Node execution failed after max retries\",\n                                **{\n                                    LC.EVENT_TYPE: \"node\",\n                                    LC.ACTION: \"node_execution_failed\",\n                                    LC.NODE_ID: node.node_id,\n                                    LC.RETRY_COUNT: attempt,\n                                    LC.MAX_RETRIES: retry_policy.max_retries,\n                                    LC.CUSTOM: {\"error\": str(e)},\n                                },\n                            )\n                        )\n                        raise e\n\n                    log.warning(\n                        **wrap_constants(\n                            message=\"Node execution failed \u2014 will retry\",\n                            **{\n                                LC.EVENT_TYPE: \"node\",\n                                LC.ACTION: \"node_retry_scheduled\",\n                                LC.NODE_ID: node.node_id,\n                                LC.RETRY_COUNT: attempt,\n                                LC.MAX_RETRIES: retry_policy.max_retries,\n                                LC.RETRY_DELAY: delay,\n                                LC.CUSTOM: {\"error\": str(e)},\n                            },\n                        )\n                    )\n\n                    await asyncio.sleep(delay)\n                    delay *= retry_policy.backoff\n                    attempt += 1\n\n    async def execute(\n        self, max_supersteps: int = 100, superstep_timeout: float = 300.0\n    ) -&gt; Optional[State]:\n        \"\"\"\n        Executes the graph up to a maximum number of supersteps.\n\n        Args:\n            max_supersteps: The maximum number of supersteps to execute. Defaults to 100.\n            superstep_timeout: The timeout (in seconds) for each superstep. Defaults to 300.0.\n\n        Returns:\n            The final state of the execution, if the graph completed successfully.\n        Raises:\n             GraphExecutionError: if the max_supersteps are reach or any error is encountered in the flow\n        \"\"\"\n        log = GraphLogger.get()\n\n        log.info(\n            **wrap_constants(\n                message=\"Graph execution started\",\n                **{\n                    LC.EVENT_TYPE: \"executor\",\n                    LC.ACTION: \"execution_start\",\n                    LC.CUSTOM: {\n                        \"max_supersteps\": max_supersteps,\n                        \"timeout_per_superstep\": superstep_timeout,\n                    },\n                },\n            )\n        )\n\n        final_state = None\n\n        while self.active_states and self.superstep &lt; max_supersteps:\n            log.info(\n                **wrap_constants(\n                    message=f\"Superstep {self.superstep} execution\",\n                    **{\n                        LC.EVENT_TYPE: \"executor\",\n                        LC.ACTION: \"superstep_started\",\n                        LC.SUPERSTEP: self.superstep,\n                        LC.CUSTOM: {\"active_nodes\": list(self.active_states.keys())},\n                    },\n                )\n            )\n\n            next_active_states: Dict[str, List[State]] = defaultdict(list)\n            tasks = []\n\n            for node_id, states in self.active_states.items():\n                node = self.graph.nodes[node_id]\n                input_data = (\n                    states\n                    if isinstance(node, AggregatorNode)\n                    else copy.deepcopy(states[0])\n                )\n\n                task = asyncio.create_task(\n                    asyncio.wait_for(\n                        self._execute_node_with_retry_async(\n                            node, input_data, self.retry_policy\n                        ),\n                        timeout=superstep_timeout,\n                    )\n                )\n                tasks.append((node_id, task, input_data))\n\n            for node_id, task, original_input in tasks:\n                node = self.graph.nodes[node_id]\n                try:\n                    result_state = await task\n                    log.info(\n                        **wrap_constants(\n                            message=\"Node execution complete\",\n                            **{\n                                LC.EVENT_TYPE: \"node\",\n                                LC.ACTION: \"node_execution_complete\",\n                                LC.SUPERSTEP: self.superstep,\n                                LC.NODE_ID: node_id,\n                            },\n                        )\n                    )\n\n                except asyncio.TimeoutError:\n                    log.error(\n                        **wrap_constants(\n                            message=\"Node execution timed out\",\n                            **{\n                                LC.EVENT_TYPE: \"node\",\n                                LC.ACTION: \"timeout\",\n                                LC.SUPERSTEP: self.superstep,\n                                LC.NODE_ID: node_id,\n                                LC.TIMEOUT: superstep_timeout,\n                            },\n                        )\n                    )\n\n                    if (\n                        self.allow_fallback_from_checkpoint\n                        and not self.already_retried_from_checkpoint\n                    ):\n                        log.warning(\n                            **wrap_constants(\n                                message=\"Falling back to checkpoint after timeout\",\n                                **{\n                                    LC.EVENT_TYPE: \"executor\",\n                                    LC.ACTION: \"fallback_to_checkpoint\",\n                                },\n                            )\n                        )\n                        chkpt = CheckpointData.load(self.checkpoint_path)\n                        fallback_executor = GraphExecutor.from_checkpoint(\n                            chkpt,\n                            checkpoint_path=self.checkpoint_path,\n                            checkpoint_every=self.checkpoint_every,\n                        )\n                        fallback_executor.allow_fallback_from_checkpoint = False\n                        fallback_executor.already_retried_from_checkpoint = True\n                        return await fallback_executor.execute(\n                            max_supersteps=max_supersteps,\n                            superstep_timeout=superstep_timeout,\n                        )\n\n                    log.error(\n                        **wrap_constants(\n                            message=\"No checkpoint fallback available\",\n                            **{\n                                LC.EVENT_TYPE: \"executor\",\n                                LC.ACTION: \"no_fallback\",\n                                LC.NODE_ID: node_id,\n                            },\n                        )\n                    )\n                    raise GraphExecutionError(\n                        node_id, f\"Execution timed out after {superstep_timeout}s.\"\n                    )\n\n                except Exception as e:\n                    fallback_id = getattr(node, \"fallback_node_id\", None)\n                    if fallback_id:\n                        fallback_node = self.graph.nodes[fallback_id]\n                        log.warning(\n                            **wrap_constants(\n                                message=\"Fallback invoked due to node failure\",\n                                **{\n                                    LC.EVENT_TYPE: \"executor\",\n                                    LC.ACTION: \"fallback_invoked\",\n                                    LC.SOURCE_NODE: node_id,\n                                    LC.FALLBACK_NODE: fallback_id,\n                                    LC.CUSTOM: {\"reason\": str(e)},\n                                },\n                            )\n                        )\n                        try:\n                            result_state = await asyncio.wait_for(\n                                self._execute_node_with_retry_async(\n                                    fallback_node, original_input, self.retry_policy\n                                ),\n                                timeout=superstep_timeout,\n                            )\n                            log.info(\n                                **wrap_constants(\n                                    message=\"Fallback node execution succeeded\",\n                                    **{\n                                        LC.EVENT_TYPE: \"executor\",\n                                        LC.ACTION: \"fallback_success\",\n                                        LC.FALLBACK_NODE: fallback_id,\n                                    },\n                                )\n                            )\n                        except Exception as fallback_error:\n                            log.error(\n                                **wrap_constants(\n                                    message=\"Fallback node execution failed\",\n                                    **{\n                                        LC.EVENT_TYPE: \"executor\",\n                                        LC.ACTION: \"fallback_failed\",\n                                        LC.FALLBACK_NODE: fallback_id,\n                                        LC.CUSTOM: {\"reason\": str(fallback_error)},\n                                    },\n                                )\n                            )\n                            raise GraphExecutionError(\n                                fallback_id, f\"Fallback node failed: {fallback_error}\"\n                            )\n                    else:\n                        log.error(\n                            **wrap_constants(\n                                message=\"Node execution failed without fallback\",\n                                **{\n                                    LC.EVENT_TYPE: \"node\",\n                                    LC.ACTION: \"node_execution_failed\",\n                                    LC.NODE_ID: node_id,\n                                    LC.SUPERSTEP: self.superstep,\n                                    LC.CUSTOM: {\"error\": str(e)},\n                                },\n                            )\n                        )\n                        raise GraphExecutionError(node_id, str(e))\n\n                # Transition state to next active nodes\n                for edge in node.outgoing_edges:\n                    if isinstance(edge, ConcreteEdge):\n                        next_active_states[edge.sink.node_id].append(\n                            copy.deepcopy(result_state)\n                        )\n                        log.info(\n                            **wrap_constants(\n                                message=\"Edge transition (concrete)\",\n                                **{\n                                    LC.EVENT_TYPE: \"edge\",\n                                    LC.ACTION: \"concrete_edge_transition\",\n                                    LC.SOURCE_NODE: node_id,\n                                    LC.SINK_NODE: edge.sink.node_id,\n                                },\n                            )\n                        )\n                    elif isinstance(edge, ConditionalEdge):\n                        chosen_id = await edge.routing_function(result_state)\n                        valid_ids = [sink.node_id for sink in edge.sinks]\n                        if chosen_id not in valid_ids:\n                            raise GraphExecutionError(\n                                node.node_id, f\"Invalid routing output: '{chosen_id}'\"\n                            )\n                        next_active_states[chosen_id].append(\n                            copy.deepcopy(result_state)\n                        )\n                        log.info(\n                            **wrap_constants(\n                                message=\"Edge transition (conditional)\",\n                                **{\n                                    LC.EVENT_TYPE: \"edge\",\n                                    LC.ACTION: \"conditional_edge_transition\",\n                                    LC.SOURCE_NODE: node_id,\n                                    LC.SINK_NODE: chosen_id,\n                                    LC.ROUTER_FUNC: edge.routing_function.__name__,\n                                },\n                            )\n                        )\n\n                if node_id == self.graph.end_node.node_id:\n                    final_state = result_state\n\n            self.active_states = next_active_states\n            self.superstep += 1\n\n            # \ud83d\udcbe Auto-checkpointing\n            if (\n                self.checkpoint_path\n                and self.checkpoint_every\n                and self.superstep % self.checkpoint_every == 0\n            ):\n                log.info(\n                    **wrap_constants(\n                        message=\"Auto-saving checkpoint\",\n                        **{\n                            LC.EVENT_TYPE: \"executor\",\n                            LC.ACTION: \"auto_checkpoint\",\n                            LC.SUPERSTEP: self.superstep,\n                            LC.CUSTOM: {\"checkpoint_path\": self.checkpoint_path},\n                        },\n                    )\n                )\n                self.to_checkpoint().save(self.checkpoint_path)\n\n            log.info(\n                **wrap_constants(\n                    message=\"Superstep completed\",\n                    **{\n                        LC.EVENT_TYPE: \"executor\",\n                        LC.ACTION: \"superstep_complete\",\n                        LC.SUPERSTEP: self.superstep,\n                        LC.CUSTOM: {\n                            \"next_active_nodes\": list(self.active_states.keys())\n                        },\n                    },\n                )\n            )\n\n        if self.superstep &gt;= max_supersteps:\n            log.error(\n                **wrap_constants(\n                    message=\"Max supersteps reached \u2014 possible infinite loop\",\n                    **{LC.EVENT_TYPE: \"executor\", LC.ACTION: \"max_supersteps_exceeded\"},\n                )\n            )\n            raise GraphExecutionError(\"N/A\", \"Max supersteps reached\")\n\n        log.info(\n            **wrap_constants(\n                message=\"Graph execution completed successfully\",\n                **{LC.EVENT_TYPE: \"executor\", LC.ACTION: \"execution_complete\"},\n            )\n        )\n\n        log.info(\n            **wrap_constants(\n                message=\"Final state summary\",\n                **{\n                    LC.EVENT_TYPE: \"executor\",\n                    LC.ACTION: \"final_state\",\n                    LC.CUSTOM: {\n                        \"message_count\": (\n                            len(final_state.messages) if final_state else None\n                        )\n                    },\n                },\n            )\n        )\n\n        return final_state\n</code></pre>"},{"location":"reference/GraphExecutor/#graphorchestrator.graph.executor.GraphExecutor.__init__","title":"<code>__init__(graph, initial_state, max_workers=4, retry_policy=None, checkpoint_path=None, checkpoint_every=None, allow_fallback_from_checkpoint=False)</code>","text":"<p>Initializes the GraphExecutor with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <p>The graph to execute.</p> required <code>initial_state</code> <p>The initial state of the graph execution.</p> required <code>max_workers</code> <code>int</code> <p>The maximum number of concurrent node executions. Defaults to 4.</p> <code>4</code> <code>retry_policy</code> <code>Optional[RetryPolicy]</code> <p>The retry policy for node executions. Defaults to no retries.</p> <code>None</code> <code>checkpoint_path</code> <code>Optional[str]</code> <p>The path to save/load checkpoints. Defaults to None.</p> <code>None</code> <code>checkpoint_every</code> <code>Optional[int]</code> <p>The frequency (in supersteps) to save checkpoints. Defaults to None.</p> <code>None</code> <code>allow_fallback_from_checkpoint</code> <code>bool</code> <p>Whether to fallback to the last checkpoint in case of timeout. Defaults to False.</p> <code>False</code> Source code in <code>graphorchestrator\\graph\\executor.py</code> <pre><code>def __init__(\n    self,\n    graph,\n    initial_state,\n    max_workers: int = 4,\n    retry_policy: Optional[RetryPolicy] = None,\n    checkpoint_path: Optional[str] = None,\n    checkpoint_every: Optional[int] = None,\n    allow_fallback_from_checkpoint: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initializes the GraphExecutor with the given parameters.\n\n    Args:\n        graph: The graph to execute.\n        initial_state: The initial state of the graph execution.\n        max_workers: The maximum number of concurrent node executions. Defaults to 4.\n        retry_policy: The retry policy for node executions. Defaults to no retries.\n        checkpoint_path: The path to save/load checkpoints. Defaults to None.\n        checkpoint_every: The frequency (in supersteps) to save checkpoints. Defaults to None.\n        allow_fallback_from_checkpoint: Whether to fallback to the last checkpoint in case of timeout. Defaults to False.\n    \"\"\"\n    LogContext.set(\n        {\n            LC.RUN_ID: str(uuid.uuid4()),\n            LC.GRAPH_NAME: getattr(graph, \"name\", None),\n            LC.USER_ID: getpass.getuser(),\n            LC.HOSTNAME: socket.gethostname(),\n        }\n    )\n    log = GraphLogger.get()\n    log.info(\n        **wrap_constants(\n            message=\"GraphExecutor initialized\",\n            **{\n                LC.EVENT_TYPE: \"executor\",\n                LC.ACTION: \"executor_init\",\n                LC.CUSTOM: {\n                    \"max_workers\": max_workers,\n                    \"checkpoint_enabled\": bool(checkpoint_path),\n                    \"checkpoint_every\": checkpoint_every,\n                    \"allow_fallback_from_checkpoint\": allow_fallback_from_checkpoint,\n                    \"retry_policy\": {\n                        \"max_retries\": (\n                            retry_policy.max_retries if retry_policy else 0\n                        ),\n                        \"delay\": retry_policy.delay if retry_policy else 0,\n                        \"backoff\": retry_policy.backoff if retry_policy else 1,\n                    },\n                },\n            },\n        )\n    )\n\n    self.graph = graph\n    self.initial_state = initial_state\n    self.max_workers = max_workers\n    self.active_states: Dict[str, List[State]] = defaultdict(list)\n    self.active_states[graph.start_node.node_id].append(initial_state)\n    self.retry_policy = (\n        retry_policy if retry_policy else RetryPolicy(max_retries=0, delay=0)\n    )\n    self.semaphore = asyncio.Semaphore(self.max_workers)\n    self.checkpoint_path = checkpoint_path\n    self.checkpoint_every = checkpoint_every\n    self.superstep = 0\n    self.final_state = None\n    self.allow_fallback_from_checkpoint = allow_fallback_from_checkpoint\n    self.already_retried_from_checkpoint = False\n\n    if self.allow_fallback_from_checkpoint and not self.checkpoint_path:\n        log.error(\n            **wrap_constants(\n                message=\"Checkpoint fallback enabled without path\",\n                **{\n                    LC.EVENT_TYPE: \"executor\",\n                    LC.ACTION: \"executor_init_failed\",\n                    LC.CUSTOM: {\n                        \"reason\": \"allow_fallback_from_checkpoint=True but checkpoint_path=None\"\n                    },\n                },\n            )\n        )\n        raise GraphExecutionError(\n            node_id=\"GraphExecutor\",\n            message=\"Fallback from checkpoint is enabled, but no checkpoint_path is provided.\",\n        )\n</code></pre>"},{"location":"reference/GraphExecutor/#graphorchestrator.graph.executor.GraphExecutor.execute","title":"<code>execute(max_supersteps=100, superstep_timeout=300.0)</code>  <code>async</code>","text":"<p>Executes the graph up to a maximum number of supersteps.</p> <p>Parameters:</p> Name Type Description Default <code>max_supersteps</code> <code>int</code> <p>The maximum number of supersteps to execute. Defaults to 100.</p> <code>100</code> <code>superstep_timeout</code> <code>float</code> <p>The timeout (in seconds) for each superstep. Defaults to 300.0.</p> <code>300.0</code> <p>Returns:</p> Type Description <code>Optional[State]</code> <p>The final state of the execution, if the graph completed successfully.</p> <p>Raises:      GraphExecutionError: if the max_supersteps are reach or any error is encountered in the flow</p> Source code in <code>graphorchestrator\\graph\\executor.py</code> <pre><code>async def execute(\n    self, max_supersteps: int = 100, superstep_timeout: float = 300.0\n) -&gt; Optional[State]:\n    \"\"\"\n    Executes the graph up to a maximum number of supersteps.\n\n    Args:\n        max_supersteps: The maximum number of supersteps to execute. Defaults to 100.\n        superstep_timeout: The timeout (in seconds) for each superstep. Defaults to 300.0.\n\n    Returns:\n        The final state of the execution, if the graph completed successfully.\n    Raises:\n         GraphExecutionError: if the max_supersteps are reach or any error is encountered in the flow\n    \"\"\"\n    log = GraphLogger.get()\n\n    log.info(\n        **wrap_constants(\n            message=\"Graph execution started\",\n            **{\n                LC.EVENT_TYPE: \"executor\",\n                LC.ACTION: \"execution_start\",\n                LC.CUSTOM: {\n                    \"max_supersteps\": max_supersteps,\n                    \"timeout_per_superstep\": superstep_timeout,\n                },\n            },\n        )\n    )\n\n    final_state = None\n\n    while self.active_states and self.superstep &lt; max_supersteps:\n        log.info(\n            **wrap_constants(\n                message=f\"Superstep {self.superstep} execution\",\n                **{\n                    LC.EVENT_TYPE: \"executor\",\n                    LC.ACTION: \"superstep_started\",\n                    LC.SUPERSTEP: self.superstep,\n                    LC.CUSTOM: {\"active_nodes\": list(self.active_states.keys())},\n                },\n            )\n        )\n\n        next_active_states: Dict[str, List[State]] = defaultdict(list)\n        tasks = []\n\n        for node_id, states in self.active_states.items():\n            node = self.graph.nodes[node_id]\n            input_data = (\n                states\n                if isinstance(node, AggregatorNode)\n                else copy.deepcopy(states[0])\n            )\n\n            task = asyncio.create_task(\n                asyncio.wait_for(\n                    self._execute_node_with_retry_async(\n                        node, input_data, self.retry_policy\n                    ),\n                    timeout=superstep_timeout,\n                )\n            )\n            tasks.append((node_id, task, input_data))\n\n        for node_id, task, original_input in tasks:\n            node = self.graph.nodes[node_id]\n            try:\n                result_state = await task\n                log.info(\n                    **wrap_constants(\n                        message=\"Node execution complete\",\n                        **{\n                            LC.EVENT_TYPE: \"node\",\n                            LC.ACTION: \"node_execution_complete\",\n                            LC.SUPERSTEP: self.superstep,\n                            LC.NODE_ID: node_id,\n                        },\n                    )\n                )\n\n            except asyncio.TimeoutError:\n                log.error(\n                    **wrap_constants(\n                        message=\"Node execution timed out\",\n                        **{\n                            LC.EVENT_TYPE: \"node\",\n                            LC.ACTION: \"timeout\",\n                            LC.SUPERSTEP: self.superstep,\n                            LC.NODE_ID: node_id,\n                            LC.TIMEOUT: superstep_timeout,\n                        },\n                    )\n                )\n\n                if (\n                    self.allow_fallback_from_checkpoint\n                    and not self.already_retried_from_checkpoint\n                ):\n                    log.warning(\n                        **wrap_constants(\n                            message=\"Falling back to checkpoint after timeout\",\n                            **{\n                                LC.EVENT_TYPE: \"executor\",\n                                LC.ACTION: \"fallback_to_checkpoint\",\n                            },\n                        )\n                    )\n                    chkpt = CheckpointData.load(self.checkpoint_path)\n                    fallback_executor = GraphExecutor.from_checkpoint(\n                        chkpt,\n                        checkpoint_path=self.checkpoint_path,\n                        checkpoint_every=self.checkpoint_every,\n                    )\n                    fallback_executor.allow_fallback_from_checkpoint = False\n                    fallback_executor.already_retried_from_checkpoint = True\n                    return await fallback_executor.execute(\n                        max_supersteps=max_supersteps,\n                        superstep_timeout=superstep_timeout,\n                    )\n\n                log.error(\n                    **wrap_constants(\n                        message=\"No checkpoint fallback available\",\n                        **{\n                            LC.EVENT_TYPE: \"executor\",\n                            LC.ACTION: \"no_fallback\",\n                            LC.NODE_ID: node_id,\n                        },\n                    )\n                )\n                raise GraphExecutionError(\n                    node_id, f\"Execution timed out after {superstep_timeout}s.\"\n                )\n\n            except Exception as e:\n                fallback_id = getattr(node, \"fallback_node_id\", None)\n                if fallback_id:\n                    fallback_node = self.graph.nodes[fallback_id]\n                    log.warning(\n                        **wrap_constants(\n                            message=\"Fallback invoked due to node failure\",\n                            **{\n                                LC.EVENT_TYPE: \"executor\",\n                                LC.ACTION: \"fallback_invoked\",\n                                LC.SOURCE_NODE: node_id,\n                                LC.FALLBACK_NODE: fallback_id,\n                                LC.CUSTOM: {\"reason\": str(e)},\n                            },\n                        )\n                    )\n                    try:\n                        result_state = await asyncio.wait_for(\n                            self._execute_node_with_retry_async(\n                                fallback_node, original_input, self.retry_policy\n                            ),\n                            timeout=superstep_timeout,\n                        )\n                        log.info(\n                            **wrap_constants(\n                                message=\"Fallback node execution succeeded\",\n                                **{\n                                    LC.EVENT_TYPE: \"executor\",\n                                    LC.ACTION: \"fallback_success\",\n                                    LC.FALLBACK_NODE: fallback_id,\n                                },\n                            )\n                        )\n                    except Exception as fallback_error:\n                        log.error(\n                            **wrap_constants(\n                                message=\"Fallback node execution failed\",\n                                **{\n                                    LC.EVENT_TYPE: \"executor\",\n                                    LC.ACTION: \"fallback_failed\",\n                                    LC.FALLBACK_NODE: fallback_id,\n                                    LC.CUSTOM: {\"reason\": str(fallback_error)},\n                                },\n                            )\n                        )\n                        raise GraphExecutionError(\n                            fallback_id, f\"Fallback node failed: {fallback_error}\"\n                        )\n                else:\n                    log.error(\n                        **wrap_constants(\n                            message=\"Node execution failed without fallback\",\n                            **{\n                                LC.EVENT_TYPE: \"node\",\n                                LC.ACTION: \"node_execution_failed\",\n                                LC.NODE_ID: node_id,\n                                LC.SUPERSTEP: self.superstep,\n                                LC.CUSTOM: {\"error\": str(e)},\n                            },\n                        )\n                    )\n                    raise GraphExecutionError(node_id, str(e))\n\n            # Transition state to next active nodes\n            for edge in node.outgoing_edges:\n                if isinstance(edge, ConcreteEdge):\n                    next_active_states[edge.sink.node_id].append(\n                        copy.deepcopy(result_state)\n                    )\n                    log.info(\n                        **wrap_constants(\n                            message=\"Edge transition (concrete)\",\n                            **{\n                                LC.EVENT_TYPE: \"edge\",\n                                LC.ACTION: \"concrete_edge_transition\",\n                                LC.SOURCE_NODE: node_id,\n                                LC.SINK_NODE: edge.sink.node_id,\n                            },\n                        )\n                    )\n                elif isinstance(edge, ConditionalEdge):\n                    chosen_id = await edge.routing_function(result_state)\n                    valid_ids = [sink.node_id for sink in edge.sinks]\n                    if chosen_id not in valid_ids:\n                        raise GraphExecutionError(\n                            node.node_id, f\"Invalid routing output: '{chosen_id}'\"\n                        )\n                    next_active_states[chosen_id].append(\n                        copy.deepcopy(result_state)\n                    )\n                    log.info(\n                        **wrap_constants(\n                            message=\"Edge transition (conditional)\",\n                            **{\n                                LC.EVENT_TYPE: \"edge\",\n                                LC.ACTION: \"conditional_edge_transition\",\n                                LC.SOURCE_NODE: node_id,\n                                LC.SINK_NODE: chosen_id,\n                                LC.ROUTER_FUNC: edge.routing_function.__name__,\n                            },\n                        )\n                    )\n\n            if node_id == self.graph.end_node.node_id:\n                final_state = result_state\n\n        self.active_states = next_active_states\n        self.superstep += 1\n\n        # \ud83d\udcbe Auto-checkpointing\n        if (\n            self.checkpoint_path\n            and self.checkpoint_every\n            and self.superstep % self.checkpoint_every == 0\n        ):\n            log.info(\n                **wrap_constants(\n                    message=\"Auto-saving checkpoint\",\n                    **{\n                        LC.EVENT_TYPE: \"executor\",\n                        LC.ACTION: \"auto_checkpoint\",\n                        LC.SUPERSTEP: self.superstep,\n                        LC.CUSTOM: {\"checkpoint_path\": self.checkpoint_path},\n                    },\n                )\n            )\n            self.to_checkpoint().save(self.checkpoint_path)\n\n        log.info(\n            **wrap_constants(\n                message=\"Superstep completed\",\n                **{\n                    LC.EVENT_TYPE: \"executor\",\n                    LC.ACTION: \"superstep_complete\",\n                    LC.SUPERSTEP: self.superstep,\n                    LC.CUSTOM: {\n                        \"next_active_nodes\": list(self.active_states.keys())\n                    },\n                },\n            )\n        )\n\n    if self.superstep &gt;= max_supersteps:\n        log.error(\n            **wrap_constants(\n                message=\"Max supersteps reached \u2014 possible infinite loop\",\n                **{LC.EVENT_TYPE: \"executor\", LC.ACTION: \"max_supersteps_exceeded\"},\n            )\n        )\n        raise GraphExecutionError(\"N/A\", \"Max supersteps reached\")\n\n    log.info(\n        **wrap_constants(\n            message=\"Graph execution completed successfully\",\n            **{LC.EVENT_TYPE: \"executor\", LC.ACTION: \"execution_complete\"},\n        )\n    )\n\n    log.info(\n        **wrap_constants(\n            message=\"Final state summary\",\n            **{\n                LC.EVENT_TYPE: \"executor\",\n                LC.ACTION: \"final_state\",\n                LC.CUSTOM: {\n                    \"message_count\": (\n                        len(final_state.messages) if final_state else None\n                    )\n                },\n            },\n        )\n    )\n\n    return final_state\n</code></pre>"},{"location":"reference/GraphExecutor/#graphorchestrator.graph.executor.GraphExecutor.from_checkpoint","title":"<code>from_checkpoint(chkpt, checkpoint_path=None, checkpoint_every=None)</code>  <code>classmethod</code>","text":"<p>Creates a GraphExecutor object from a CheckpointData object.</p> <p>Parameters:</p> Name Type Description Default <code>chkpt</code> <code>CheckpointData</code> <p>The CheckpointData object to restore from.</p> required <code>checkpoint_path</code> <code>Optional[str]</code> <p>The path to save/load checkpoints. Defaults to None.</p> <code>None</code> <code>checkpoint_every</code> <code>Optional[int]</code> <p>The frequency (in supersteps) to save checkpoints. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>A GraphExecutor object restored from the checkpoint.</p> Source code in <code>graphorchestrator\\graph\\executor.py</code> <pre><code>@classmethod\ndef from_checkpoint(\n    cls,\n    chkpt: CheckpointData,\n    checkpoint_path: Optional[str] = None,\n    checkpoint_every: Optional[int] = None,\n):\n    \"\"\"\n    Creates a GraphExecutor object from a CheckpointData object.\n\n    Args:\n        chkpt: The CheckpointData object to restore from.\n        checkpoint_path: The path to save/load checkpoints. Defaults to None.\n        checkpoint_every: The frequency (in supersteps) to save checkpoints. Defaults to None.\n\n    Returns:\n        A GraphExecutor object restored from the checkpoint.\n\n    \"\"\"\n    log = GraphLogger.get()\n\n    log.info(\n        **wrap_constants(\n            message=\"Restoring executor from checkpoint\",\n            **{\n                LC.EVENT_TYPE: \"executor\",\n                LC.ACTION: \"restore_from_checkpoint\",\n                LC.SUPERSTEP: chkpt.superstep,\n                LC.CUSTOM: {\n                    \"active_node_ids\": list(chkpt.active_states.keys()),\n                    \"final_state_message_count\": (\n                        len(chkpt.final_state.messages)\n                        if chkpt.final_state\n                        else None\n                    ),\n                    \"max_workers\": chkpt.max_workers,\n                    \"checkpoint_path\": checkpoint_path,\n                    \"checkpoint_every\": checkpoint_every,\n                },\n            },\n        )\n    )\n\n    executor = cls(\n        graph=chkpt.graph,\n        initial_state=chkpt.initial_state,\n        max_workers=chkpt.max_workers,\n        retry_policy=chkpt.retry_policy,\n        checkpoint_path=checkpoint_path,\n        checkpoint_every=checkpoint_every,\n    )\n    executor.active_states = chkpt.active_states\n    executor.superstep = chkpt.superstep\n    executor.final_state = chkpt.final_state\n    return executor\n</code></pre>"},{"location":"reference/GraphExecutor/#graphorchestrator.graph.executor.GraphExecutor.to_checkpoint","title":"<code>to_checkpoint()</code>","text":"<p>Creates a CheckpointData object representing the current state of the graph execution.</p> <p>Returns:</p> Type Description <code>CheckpointData</code> <p>A CheckpointData object.</p> Source code in <code>graphorchestrator\\graph\\executor.py</code> <pre><code>def to_checkpoint(self) -&gt; CheckpointData:\n    \"\"\"\n    Creates a CheckpointData object representing the current state of the graph execution.\n\n    Returns:\n        A CheckpointData object.\n    \"\"\"\n    log = GraphLogger.get()\n\n    log.info(\n        **wrap_constants(\n            message=\"Serializing current graph state into checkpoint\",\n            **{\n                LC.EVENT_TYPE: \"executor\",\n                LC.ACTION: \"create_checkpoint\",\n                LC.SUPERSTEP: self.superstep,\n                LC.CUSTOM: {\n                    \"active_node_ids\": list(self.active_states.keys()),\n                    \"final_state_message_count\": (\n                        len(self.final_state.messages) if self.final_state else None\n                    ),\n                    \"max_workers\": self.max_workers,\n                    \"retry_policy\": {\n                        \"max_retries\": self.retry_policy.max_retries,\n                        \"delay\": self.retry_policy.delay,\n                        \"backoff\": self.retry_policy.backoff,\n                    },\n                },\n            },\n        )\n    )\n\n    return CheckpointData(\n        graph=self.graph,\n        initial_state=self.initial_state,\n        active_states=self.active_states,\n        superstep=self.superstep,\n        final_state=self.final_state,\n        retry_policy=self.retry_policy,\n        max_workers=self.max_workers,\n    )\n</code></pre>"},{"location":"reference/ToolSetServer/","title":"ToolSet Server","text":""},{"location":"reference/ToolSetServer/#graphorchestrator.toolsetserver.runtime","title":"<code>graphorchestrator.toolsetserver.runtime</code>","text":""},{"location":"reference/ToolSetServer/#graphorchestrator.toolsetserver.runtime.StateModel","title":"<code>StateModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the state model for messages.</p> <p>Attributes:</p> Name Type Description <code>messages</code> <code>List[Any]</code> <p>A list to store messages of any type.</p> Source code in <code>graphorchestrator\\toolsetserver\\runtime.py</code> <pre><code>class StateModel(BaseModel):\n    \"\"\"\n    Represents the state model for messages.\n\n    Attributes:\n        messages (List[Any]): A list to store messages of any type.\n    \"\"\"\n\n    messages: List[Any] = Field(default_factory=list)\n</code></pre>"},{"location":"reference/ToolSetServer/#graphorchestrator.toolsetserver.runtime.ToolSetServer","title":"<code>ToolSetServer</code>","text":"<p>Base class for creating a tool set server.</p> <p>This class handles the creation of FastAPI endpoints for registered tool methods.</p> Source code in <code>graphorchestrator\\toolsetserver\\runtime.py</code> <pre><code>class ToolSetServer(metaclass=_ToolSetMeta):\n    \"\"\"\n    Base class for creating a tool set server.\n\n    This class handles the creation of FastAPI endpoints for registered tool methods.\n    \"\"\"\n\n    host: str = \"127.0.0.1\"\n    port: int = 8000\n    name: str = \"ToolSet\"\n    require_auth: bool = False\n\n    @classmethod\n    # Class method to handle authentication.\n    def authenticate(cls, token: str) -&gt; bool:\n        \"\"\"\n        Authenticates a request based on the provided token.\n\n        Args:\n            token (str): The authentication token.\n\n        Returns:\n            bool: True if the token is valid, False otherwise.\n        \"\"\"\n        return False\n\n    @classmethod\n    def serve(cls, **uvicorn_kwargs: Any):\n        \"\"\"\n        Starts the FastAPI server synchronously.\n\n        Args:\n            **uvicorn_kwargs: Keyword arguments to pass to uvicorn.run.\n\n        Raises:\n            RuntimeError: If there's an error during server start-up.\n        \"\"\"\n        uvicorn.run(\n            cls._fastapi,\n            host=uvicorn_kwargs.pop(\"host\", cls.host),\n            port=uvicorn_kwargs.pop(\"port\", cls.port),\n            log_level=uvicorn_kwargs.pop(\"log_level\", \"info\"),\n            **uvicorn_kwargs,\n        )\n\n    @classmethod\n    async def serve_async(cls, **uvicorn_kwargs: Any):\n        \"\"\"\n        Starts the FastAPI server asynchronously.\n\n        Args:\n            **uvicorn_kwargs: Keyword arguments to pass to uvicorn.run.\n        Raises:\n            RuntimeError: If there's an error during server start-up.\n        \"\"\"\n        config = uvicorn.Config(\n            cls._fastapi,\n            host=uvicorn_kwargs.pop(\"host\", cls.host),\n            port=uvicorn_kwargs.pop(\"port\", cls.port),\n            log_level=uvicorn_kwargs.pop(\"log_level\", \"info\"),\n            **uvicorn_kwargs,\n        )\n        server = uvicorn.Server(config)\n        await server.serve()\n</code></pre>"},{"location":"reference/ToolSetServer/#graphorchestrator.toolsetserver.runtime.ToolSetServer.authenticate","title":"<code>authenticate(token)</code>  <code>classmethod</code>","text":"<p>Authenticates a request based on the provided token.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The authentication token.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the token is valid, False otherwise.</p> Source code in <code>graphorchestrator\\toolsetserver\\runtime.py</code> <pre><code>@classmethod\n# Class method to handle authentication.\ndef authenticate(cls, token: str) -&gt; bool:\n    \"\"\"\n    Authenticates a request based on the provided token.\n\n    Args:\n        token (str): The authentication token.\n\n    Returns:\n        bool: True if the token is valid, False otherwise.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/ToolSetServer/#graphorchestrator.toolsetserver.runtime.ToolSetServer.serve","title":"<code>serve(**uvicorn_kwargs)</code>  <code>classmethod</code>","text":"<p>Starts the FastAPI server synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>**uvicorn_kwargs</code> <code>Any</code> <p>Keyword arguments to pass to uvicorn.run.</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there's an error during server start-up.</p> Source code in <code>graphorchestrator\\toolsetserver\\runtime.py</code> <pre><code>@classmethod\ndef serve(cls, **uvicorn_kwargs: Any):\n    \"\"\"\n    Starts the FastAPI server synchronously.\n\n    Args:\n        **uvicorn_kwargs: Keyword arguments to pass to uvicorn.run.\n\n    Raises:\n        RuntimeError: If there's an error during server start-up.\n    \"\"\"\n    uvicorn.run(\n        cls._fastapi,\n        host=uvicorn_kwargs.pop(\"host\", cls.host),\n        port=uvicorn_kwargs.pop(\"port\", cls.port),\n        log_level=uvicorn_kwargs.pop(\"log_level\", \"info\"),\n        **uvicorn_kwargs,\n    )\n</code></pre>"},{"location":"reference/ToolSetServer/#graphorchestrator.toolsetserver.runtime.ToolSetServer.serve_async","title":"<code>serve_async(**uvicorn_kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Starts the FastAPI server asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>**uvicorn_kwargs</code> <code>Any</code> <p>Keyword arguments to pass to uvicorn.run.</p> <code>{}</code> <p>Raises:     RuntimeError: If there's an error during server start-up.</p> Source code in <code>graphorchestrator\\toolsetserver\\runtime.py</code> <pre><code>@classmethod\nasync def serve_async(cls, **uvicorn_kwargs: Any):\n    \"\"\"\n    Starts the FastAPI server asynchronously.\n\n    Args:\n        **uvicorn_kwargs: Keyword arguments to pass to uvicorn.run.\n    Raises:\n        RuntimeError: If there's an error during server start-up.\n    \"\"\"\n    config = uvicorn.Config(\n        cls._fastapi,\n        host=uvicorn_kwargs.pop(\"host\", cls.host),\n        port=uvicorn_kwargs.pop(\"port\", cls.port),\n        log_level=uvicorn_kwargs.pop(\"log_level\", \"info\"),\n        **uvicorn_kwargs,\n    )\n    server = uvicorn.Server(config)\n    await server.serve()\n</code></pre>"},{"location":"reference/core/Checkpoint/","title":"Checkpoint","text":""},{"location":"reference/core/Checkpoint/#graphorchestrator.core.checkpoint","title":"<code>graphorchestrator.core.checkpoint</code>","text":""},{"location":"reference/core/Checkpoint/#graphorchestrator.core.checkpoint.CheckpointData","title":"<code>CheckpointData</code>","text":"<p>Represents the data to be checkpointed, including graph state, and execution metadata.</p> Source code in <code>graphorchestrator\\core\\checkpoint.py</code> <pre><code>class CheckpointData:\n    \"\"\"Represents the data to be checkpointed, including graph state, and execution metadata.\"\"\"\n\n    def __init__(\n        self,\n        graph: Graph,\n        initial_state: State,\n        active_states: Dict[str, List[State]],\n        superstep: int,\n        final_state: Optional[State],\n        retry_policy: RetryPolicy,\n        max_workers: int,\n    ):\n        \"\"\"\n        Initializes the CheckpointData with the necessary components for checkpointing.\n\n        Args:\n            graph (Graph): The graph structure.\n            initial_state (State): The initial state of the graph execution.\n            active_states (Dict[str, List[State]]): The states of active nodes.\n            superstep (int): The current superstep number.\n            final_state (Optional[State]): The final state of the graph execution, if available.\n            retry_policy (RetryPolicy): The retry policy applied to the graph execution.\n            max_workers (int): The maximum number of workers used in execution.\n        \"\"\"\n\n        # Assign the provided parameters to the object's attributes.\n        self.graph = graph\n        self.initial_state = initial_state\n        self.active_states = active_states\n        self.superstep = superstep\n        self.final_state = final_state\n        self.retry_policy = retry_policy\n        self.max_workers = max_workers\n\n    def save(self, path: str) -&gt; None:\n        \"\"\"\n        Serializes and saves the checkpoint data to the specified path.\n\n        Args:\n            path (str): The file path where the checkpoint will be saved.\n        \"\"\"\n\n        # Get the graph logger instance.\n        log = GraphLogger.get()\n\n        # Serialize the checkpoint data and save it to the specified file path.\n        with open(path, \"wb\") as f:\n            pickle.dump(self, f)\n\n        log.info(\n            **wrap_constants(\n                message=\"Checkpoint saved to disk\",\n                level=\"INFO\",\n                # Prepare log entry with essential checkpointing information.\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"checkpoint_save\",\n                    LC.SUPERSTEP: self.superstep,\n                    LC.CUSTOM: {\n                        \"path\": path,\n                        \"final_state_message_count\": (\n                            len(self.final_state.messages) if self.final_state else None\n                        ),\n                        \"active_nodes\": list(self.active_states.keys()),\n                    },\n                }\n            )\n        )\n\n    @staticmethod\n    def load(path: str) -&gt; \"CheckpointData\":\n        \"\"\"\n        Loads checkpoint data from the specified path.\n\n        Args:\n            path (str): The file path from which to load the checkpoint.\n\n        Returns:\n            CheckpointData: The loaded checkpoint data.\n        \"\"\"\n        # Get the graph logger instance.\n        log = GraphLogger.get()\n        # Deserialize the checkpoint data from the specified file path.\n        with open(path, \"rb\") as f:\n            data: CheckpointData = pickle.load(f)\n\n        log.info(\n            **wrap_constants(\n                message=\"Checkpoint loaded from disk\",\n                level=\"INFO\",\n                # Prepare log entry with essential checkpoint loading information.\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"checkpoint_load\",\n                    LC.SUPERSTEP: data.superstep,\n                    LC.CUSTOM: {\n                        \"path\": path,\n                        \"active_nodes\": list(data.active_states.keys()),\n                    },\n                }\n            )\n        )\n        # Return the loaded checkpoint data.\n        return data\n</code></pre>"},{"location":"reference/core/Checkpoint/#graphorchestrator.core.checkpoint.CheckpointData.__init__","title":"<code>__init__(graph, initial_state, active_states, superstep, final_state, retry_policy, max_workers)</code>","text":"<p>Initializes the CheckpointData with the necessary components for checkpointing.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph</code> <p>The graph structure.</p> required <code>initial_state</code> <code>State</code> <p>The initial state of the graph execution.</p> required <code>active_states</code> <code>Dict[str, List[State]]</code> <p>The states of active nodes.</p> required <code>superstep</code> <code>int</code> <p>The current superstep number.</p> required <code>final_state</code> <code>Optional[State]</code> <p>The final state of the graph execution, if available.</p> required <code>retry_policy</code> <code>RetryPolicy</code> <p>The retry policy applied to the graph execution.</p> required <code>max_workers</code> <code>int</code> <p>The maximum number of workers used in execution.</p> required Source code in <code>graphorchestrator\\core\\checkpoint.py</code> <pre><code>def __init__(\n    self,\n    graph: Graph,\n    initial_state: State,\n    active_states: Dict[str, List[State]],\n    superstep: int,\n    final_state: Optional[State],\n    retry_policy: RetryPolicy,\n    max_workers: int,\n):\n    \"\"\"\n    Initializes the CheckpointData with the necessary components for checkpointing.\n\n    Args:\n        graph (Graph): The graph structure.\n        initial_state (State): The initial state of the graph execution.\n        active_states (Dict[str, List[State]]): The states of active nodes.\n        superstep (int): The current superstep number.\n        final_state (Optional[State]): The final state of the graph execution, if available.\n        retry_policy (RetryPolicy): The retry policy applied to the graph execution.\n        max_workers (int): The maximum number of workers used in execution.\n    \"\"\"\n\n    # Assign the provided parameters to the object's attributes.\n    self.graph = graph\n    self.initial_state = initial_state\n    self.active_states = active_states\n    self.superstep = superstep\n    self.final_state = final_state\n    self.retry_policy = retry_policy\n    self.max_workers = max_workers\n</code></pre>"},{"location":"reference/core/Checkpoint/#graphorchestrator.core.checkpoint.CheckpointData.load","title":"<code>load(path)</code>  <code>staticmethod</code>","text":"<p>Loads checkpoint data from the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path from which to load the checkpoint.</p> required <p>Returns:</p> Name Type Description <code>CheckpointData</code> <code>CheckpointData</code> <p>The loaded checkpoint data.</p> Source code in <code>graphorchestrator\\core\\checkpoint.py</code> <pre><code>@staticmethod\ndef load(path: str) -&gt; \"CheckpointData\":\n    \"\"\"\n    Loads checkpoint data from the specified path.\n\n    Args:\n        path (str): The file path from which to load the checkpoint.\n\n    Returns:\n        CheckpointData: The loaded checkpoint data.\n    \"\"\"\n    # Get the graph logger instance.\n    log = GraphLogger.get()\n    # Deserialize the checkpoint data from the specified file path.\n    with open(path, \"rb\") as f:\n        data: CheckpointData = pickle.load(f)\n\n    log.info(\n        **wrap_constants(\n            message=\"Checkpoint loaded from disk\",\n            level=\"INFO\",\n            # Prepare log entry with essential checkpoint loading information.\n            **{\n                LC.EVENT_TYPE: \"graph\",\n                LC.ACTION: \"checkpoint_load\",\n                LC.SUPERSTEP: data.superstep,\n                LC.CUSTOM: {\n                    \"path\": path,\n                    \"active_nodes\": list(data.active_states.keys()),\n                },\n            }\n        )\n    )\n    # Return the loaded checkpoint data.\n    return data\n</code></pre>"},{"location":"reference/core/Checkpoint/#graphorchestrator.core.checkpoint.CheckpointData.save","title":"<code>save(path)</code>","text":"<p>Serializes and saves the checkpoint data to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path where the checkpoint will be saved.</p> required Source code in <code>graphorchestrator\\core\\checkpoint.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    \"\"\"\n    Serializes and saves the checkpoint data to the specified path.\n\n    Args:\n        path (str): The file path where the checkpoint will be saved.\n    \"\"\"\n\n    # Get the graph logger instance.\n    log = GraphLogger.get()\n\n    # Serialize the checkpoint data and save it to the specified file path.\n    with open(path, \"wb\") as f:\n        pickle.dump(self, f)\n\n    log.info(\n        **wrap_constants(\n            message=\"Checkpoint saved to disk\",\n            level=\"INFO\",\n            # Prepare log entry with essential checkpointing information.\n            **{\n                LC.EVENT_TYPE: \"graph\",\n                LC.ACTION: \"checkpoint_save\",\n                LC.SUPERSTEP: self.superstep,\n                LC.CUSTOM: {\n                    \"path\": path,\n                    \"final_state_message_count\": (\n                        len(self.final_state.messages) if self.final_state else None\n                    ),\n                    \"active_nodes\": list(self.active_states.keys()),\n                },\n            }\n        )\n    )\n</code></pre>"},{"location":"reference/core/Exceptions/","title":"Exceptions","text":""},{"location":"reference/core/Exceptions/#graphorchestrator.core.exceptions","title":"<code>graphorchestrator.core.exceptions</code>","text":""},{"location":"reference/core/Exceptions/#graphorchestrator.core.exceptions.AggregatorActionNotDecorated","title":"<code>AggregatorActionNotDecorated</code>","text":"<p>               Bases: <code>GraphOrchestratorException</code></p> <p>Exception raised when an aggregator action function is not decorated with @aggregator_action. It logs an error message with the name of the undecorated function.</p> Source code in <code>graphorchestrator\\core\\exceptions.py</code> <pre><code>class AggregatorActionNotDecorated(GraphOrchestratorException):\n    \"\"\"\n    Exception raised when an aggregator action function is not decorated with @aggregator_action.\n    It logs an error message with the name of the undecorated function.\n    \"\"\"\n\n    def __init__(self, func):\n        name = getattr(func, \"__name__\", repr(func))\n        msg = f\"The function '{name}' passed to Aggregator must be decorated with @aggregator_action\"\n        GraphLogger.get().error(\n            msg,\n            **wrap_constants(\n                message=msg,\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.ACTION: \"missing_aggregator_decorator\",\n                    LC.CUSTOM: {\"function\": name},\n                },\n            ),\n        )\n        super().__init__(msg)\n</code></pre>"},{"location":"reference/core/Exceptions/#graphorchestrator.core.exceptions.DuplicateNodeError","title":"<code>DuplicateNodeError</code>","text":"<p>               Bases: <code>GraphOrchestratorException</code></p> <p>Exception raised when a node with a duplicate ID is added to the graph. It logs an error message with relevant details.</p> Source code in <code>graphorchestrator\\core\\exceptions.py</code> <pre><code>class DuplicateNodeError(GraphOrchestratorException):\n    \"\"\"\n    Exception raised when a node with a duplicate ID is added to the graph.\n    It logs an error message with relevant details.\n    \"\"\"\n\n    def __init__(self, node_id: str):\n        msg = f\"Node with id '{node_id}' already exists.\"\n        GraphLogger.get().error(\n            msg,\n            **wrap_constants(\n                message=msg,\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"duplicate_node\",\n                    LC.NODE_ID: node_id,\n                },\n            ),\n        )\n        super().__init__(msg)\n        self.node_id = node_id\n</code></pre>"},{"location":"reference/core/Exceptions/#graphorchestrator.core.exceptions.EdgeExistsError","title":"<code>EdgeExistsError</code>","text":"<p>               Bases: <code>GraphOrchestratorException</code></p> <p>Exception raised when an attempt is made to create a duplicate edge in the graph. It logs an error message with information about the source and sink nodes.</p> Source code in <code>graphorchestrator\\core\\exceptions.py</code> <pre><code>class EdgeExistsError(GraphOrchestratorException):\n    \"\"\"\n    Exception raised when an attempt is made to create a duplicate edge in the graph.\n    It logs an error message with information about the source and sink nodes.\n    \"\"\"\n\n    def __init__(self, source_id: str, sink_id: str):\n        msg = f\"Edge from '{source_id}' to '{sink_id}' already exists.\"\n        GraphLogger.get().error(\n            msg,\n            **wrap_constants(\n                message=msg,\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"duplicate_edge\",\n                    LC.SOURCE_NODE: source_id,\n                    LC.SINK_NODE: sink_id,\n                },\n            ),\n        )\n        super().__init__(msg)\n        self.source_id = source_id\n        self.sink_id = sink_id\n</code></pre>"},{"location":"reference/core/Exceptions/#graphorchestrator.core.exceptions.EmptyToolNodeDescriptionError","title":"<code>EmptyToolNodeDescriptionError</code>","text":"<p>               Bases: <code>GraphOrchestratorException</code></p> <p>Exception raised when a tool function lacks a description or docstring. It logs an error message indicating the missing description.</p> Source code in <code>graphorchestrator\\core\\exceptions.py</code> <pre><code>class EmptyToolNodeDescriptionError(GraphOrchestratorException):\n    \"\"\"\n    Exception raised when a tool function lacks a description or docstring.\n    It logs an error message indicating the missing description.\n    \"\"\"\n\n    def __init__(self, func):\n        name = getattr(func, \"__name__\", repr(func))\n        msg = f\"The tool function '{name}' has no description or docstring provided\"\n        GraphLogger.get().error(\n            msg,\n            **wrap_constants(\n                message=msg,\n                **{\n                    LC.EVENT_TYPE: \"tool\",\n                    LC.ACTION: \"missing_description\",\n                    LC.CUSTOM: {\"function\": name},\n                },\n            ),\n        )\n        super().__init__(msg)\n</code></pre>"},{"location":"reference/core/Exceptions/#graphorchestrator.core.exceptions.GraphConfigurationError","title":"<code>GraphConfigurationError</code>","text":"<p>               Bases: <code>GraphOrchestratorException</code></p> <p>Exception raised when there's an error in the graph's configuration. It logs an error message with the details of the configuration issue.</p> Source code in <code>graphorchestrator\\core\\exceptions.py</code> <pre><code>class GraphConfigurationError(GraphOrchestratorException):\n    \"\"\"\n    Exception raised when there's an error in the graph's configuration.\n    It logs an error message with the details of the configuration issue.\n    \"\"\"\n\n    def __init__(self, message: str):\n        msg = f\"Graph configuration error: {message}\"\n        GraphLogger.get().error(\n            msg,\n            **wrap_constants(\n                message=msg,\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"graph_config_error\",\n                    LC.CUSTOM: {\"error\": message},\n                },\n            ),\n        )\n        super().__init__(msg)\n</code></pre>"},{"location":"reference/core/Exceptions/#graphorchestrator.core.exceptions.GraphExecutionError","title":"<code>GraphExecutionError</code>","text":"<p>               Bases: <code>GraphOrchestratorException</code></p> <p>Exception raised when an error occurs during the graph's execution. It logs an error message with details about the failed node and the reason.</p> Source code in <code>graphorchestrator\\core\\exceptions.py</code> <pre><code>class GraphExecutionError(GraphOrchestratorException):\n    \"\"\"\n    Exception raised when an error occurs during the graph's execution.\n    It logs an error message with details about the failed node and the reason.\n    \"\"\"\n\n    def __init__(self, node_id: str, message: str):\n        msg = f\"Execution failed at node '{node_id}': {message}\"\n        GraphLogger.get().error(\n            msg,\n            **wrap_constants(\n                message=msg,\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.ACTION: \"execution_failed\",\n                    LC.NODE_ID: node_id,\n                    LC.CUSTOM: {\"reason\": message},\n                },\n            ),\n        )\n        super().__init__(msg)\n        self.node_id = node_id\n        self.message = message\n</code></pre>"},{"location":"reference/core/Exceptions/#graphorchestrator.core.exceptions.GraphOrchestratorException","title":"<code>GraphOrchestratorException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for all exceptions raised by the graph orchestrator. It serves as the root of the custom exception hierarchy for this project.</p> Source code in <code>graphorchestrator\\core\\exceptions.py</code> <pre><code>class GraphOrchestratorException(Exception):\n    \"\"\"\n    Base exception class for all exceptions raised by the graph orchestrator.\n    It serves as the root of the custom exception hierarchy for this project.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/core/Exceptions/#graphorchestrator.core.exceptions.InvalidAIActionOutput","title":"<code>InvalidAIActionOutput</code>","text":"<p>               Bases: <code>GraphOrchestratorException</code></p> <p>Exception raised when an AI action returns an invalid type. It logs an error message with information about the invalid return type.</p> Source code in <code>graphorchestrator\\core\\exceptions.py</code> <pre><code>class InvalidAIActionOutput(GraphOrchestratorException):\n    \"\"\"\n    Exception raised when an AI action returns an invalid type.\n    It logs an error message with information about the invalid return type.\n    \"\"\"\n\n    def __init__(self, returned_value):\n        msg = f\"AI action must return a state, but got {type(returned_value).__name__}\"\n        GraphLogger.get().error(\n            msg,\n            **wrap_constants(\n                message=msg,\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.ACTION: \"invalid_ai_output\",\n                    LC.CUSTOM: {\"returned_type\": str(type(returned_value))},\n                },\n            ),\n        )\n        super().__init__(msg)\n</code></pre>"},{"location":"reference/core/Exceptions/#graphorchestrator.core.exceptions.InvalidAggregatorActionError","title":"<code>InvalidAggregatorActionError</code>","text":"<p>               Bases: <code>GraphOrchestratorException</code></p> <p>Exception raised when an aggregator action returns an invalid type. It logs an error message with the invalid return type.</p> Source code in <code>graphorchestrator\\core\\exceptions.py</code> <pre><code>class InvalidAggregatorActionError(GraphOrchestratorException):\n    \"\"\"\n    Exception raised when an aggregator action returns an invalid type.\n    It logs an error message with the invalid return type.\n    \"\"\"\n\n    def __init__(self, returned_value):\n        msg = f\"Aggregator action must return a state, but got {type(returned_value).__name__}\"\n        GraphLogger.get().error(\n            msg,\n            **wrap_constants(\n                message=msg,\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.ACTION: \"invalid_aggregator_output\",\n                    LC.CUSTOM: {\"returned_type\": str(type(returned_value))},\n                },\n            ),\n        )\n        super().__init__(msg)\n</code></pre>"},{"location":"reference/core/Exceptions/#graphorchestrator.core.exceptions.InvalidNodeActionOutput","title":"<code>InvalidNodeActionOutput</code>","text":"<p>               Bases: <code>GraphOrchestratorException</code></p> <p>Exception raised when a node's action returns an invalid type. It logs an error message with information about the invalid return type.</p> Source code in <code>graphorchestrator\\core\\exceptions.py</code> <pre><code>class InvalidNodeActionOutput(GraphOrchestratorException):\n    \"\"\"\n    Exception raised when a node's action returns an invalid type.\n    It logs an error message with information about the invalid return type.\n    \"\"\"\n\n    def __init__(self, returned_value):\n        msg = f\"Node action must return a state, but got {type(returned_value).__name__}: {returned_value}\"\n        GraphLogger.get().error(\n            msg,\n            **wrap_constants(\n                message=msg,\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.ACTION: \"invalid_node_output\",\n                    LC.CUSTOM: {\n                        \"returned_type\": str(type(returned_value)),\n                        \"value\": str(returned_value)[:100],\n                    },\n                },\n            ),\n        )\n        super().__init__(msg)\n</code></pre>"},{"location":"reference/core/Exceptions/#graphorchestrator.core.exceptions.InvalidRoutingFunctionOutput","title":"<code>InvalidRoutingFunctionOutput</code>","text":"<p>               Bases: <code>GraphOrchestratorException</code></p> <p>Exception raised when a routing function returns an invalid type. It logs an error message with information about the invalid return type.</p> Source code in <code>graphorchestrator\\core\\exceptions.py</code> <pre><code>class InvalidRoutingFunctionOutput(GraphOrchestratorException):\n    \"\"\"\n    Exception raised when a routing function returns an invalid type.\n    It logs an error message with information about the invalid return type.\n    \"\"\"\n\n    def __init__(self, returned_value):\n        msg = f\"Routing function must return a string, but got {type(returned_value).__name__}: {returned_value}\"\n        GraphLogger.get().error(\n            msg,\n            **wrap_constants(\n                message=msg,\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.ACTION: \"invalid_routing_output\",\n                    LC.CUSTOM: {\n                        \"returned_type\": str(type(returned_value)),\n                        \"value\": str(returned_value)[:100],\n                    },\n                },\n            ),\n        )\n        super().__init__(msg)\n</code></pre>"},{"location":"reference/core/Exceptions/#graphorchestrator.core.exceptions.InvalidToolMethodOutput","title":"<code>InvalidToolMethodOutput</code>","text":"<p>               Bases: <code>GraphOrchestratorException</code></p> <p>Exception raised when a tool method returns an invalid type. It logs an error message with information about the invalid return type.</p> Source code in <code>graphorchestrator\\core\\exceptions.py</code> <pre><code>class InvalidToolMethodOutput(GraphOrchestratorException):\n    \"\"\"\n    Exception raised when a tool method returns an invalid type.\n    It logs an error message with information about the invalid return type.\n    \"\"\"\n\n    def __init__(self, returned_value):\n        msg = f\"Tool method must return a state, but got {type(returned_value).__name__}: {returned_value}\"\n        GraphLogger.get().error(\n            msg,\n            **wrap_constants(\n                message=msg,\n                **{\n                    LC.EVENT_TYPE: \"tool\",\n                    LC.ACTION: \"invalid_tool_output\",\n                    LC.CUSTOM: {\n                        \"returned_type\": str(type(returned_value)),\n                        \"value\": str(returned_value)[:100],\n                    },\n                },\n            ),\n        )\n        super().__init__(msg)\n</code></pre>"},{"location":"reference/core/Exceptions/#graphorchestrator.core.exceptions.NodeActionNotDecoratedError","title":"<code>NodeActionNotDecoratedError</code>","text":"<p>               Bases: <code>GraphOrchestratorException</code></p> <p>Exception raised when a node's action function is not decorated with @node_action. It logs an error message with the name of the undecorated function.</p> Source code in <code>graphorchestrator\\core\\exceptions.py</code> <pre><code>class NodeActionNotDecoratedError(GraphOrchestratorException):\n    \"\"\"\n    Exception raised when a node's action function is not decorated with @node_action.\n    It logs an error message with the name of the undecorated function.\n    \"\"\"\n\n    def __init__(self, func):\n        name = getattr(func, \"__name__\", repr(func))\n        msg = f\"The function '{name}' passed to ProcessingNode must be decorated with @node_action.\"\n        GraphLogger.get().error(\n            msg,\n            **wrap_constants(\n                message=msg,\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.ACTION: \"missing_node_action_decorator\",\n                    LC.CUSTOM: {\"function\": name},\n                },\n            ),\n        )\n        super().__init__(msg)\n</code></pre>"},{"location":"reference/core/Exceptions/#graphorchestrator.core.exceptions.NodeNotFoundError","title":"<code>NodeNotFoundError</code>","text":"<p>               Bases: <code>GraphOrchestratorException</code></p> <p>Exception raised when a node is not found in the graph. It logs an error message with the ID of the missing node.</p> Source code in <code>graphorchestrator\\core\\exceptions.py</code> <pre><code>class NodeNotFoundError(GraphOrchestratorException):\n    \"\"\"\n    Exception raised when a node is not found in the graph.\n    It logs an error message with the ID of the missing node.\n    \"\"\"\n\n    def __init__(self, node_id: str):\n        msg = f\"Node '{node_id}' not found in the graph.\"\n        GraphLogger.get().error(\n            msg,\n            **wrap_constants(\n                message=msg,\n                **{\n                    LC.EVENT_TYPE: \"graph\",\n                    LC.ACTION: \"node_not_found\",\n                    LC.NODE_ID: node_id,\n                },\n            ),\n        )\n        super().__init__(msg)\n        self.node_id = node_id\n</code></pre>"},{"location":"reference/core/Exceptions/#graphorchestrator.core.exceptions.RoutingFunctionNotDecoratedError","title":"<code>RoutingFunctionNotDecoratedError</code>","text":"<p>               Bases: <code>GraphOrchestratorException</code></p> <p>Exception raised when a routing function is not decorated with @routing_function. It logs an error message with the name of the undecorated function.</p> Source code in <code>graphorchestrator\\core\\exceptions.py</code> <pre><code>class RoutingFunctionNotDecoratedError(GraphOrchestratorException):\n    \"\"\"\n    Exception raised when a routing function is not decorated with @routing_function.\n    It logs an error message with the name of the undecorated function.\n    \"\"\"\n\n    def __init__(self, func):\n        name = getattr(func, \"__name__\", repr(func))\n        msg = f\"The function '{name}' passed to ConditionalEdge must be decorated with @routing_function.\"\n        GraphLogger.get().error(\n            msg,\n            **wrap_constants(\n                message=msg,\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.ACTION: \"missing_routing_function_decorator\",\n                    LC.CUSTOM: {\"function\": name},\n                },\n            ),\n        )\n        super().__init__(msg)\n</code></pre>"},{"location":"reference/core/Exceptions/#graphorchestrator.core.exceptions.ToolMethodNotDecorated","title":"<code>ToolMethodNotDecorated</code>","text":"<p>               Bases: <code>GraphOrchestratorException</code></p> <p>Exception raised when a tool method is not decorated with @tool_method. It logs an error message with the name of the undecorated method.</p> Source code in <code>graphorchestrator\\core\\exceptions.py</code> <pre><code>class ToolMethodNotDecorated(GraphOrchestratorException):\n    \"\"\"\n    Exception raised when a tool method is not decorated with @tool_method.\n    It logs an error message with the name of the undecorated method.\n    \"\"\"\n\n    def __init__(self, func):\n        name = getattr(func, \"__name__\", repr(func))\n        msg = f\"The function '{name}' passed to ToolNode has to be decorated with @tool_method\"\n        GraphLogger.get().error(\n            msg,\n            **wrap_constants(\n                message=msg,\n                **{\n                    LC.EVENT_TYPE: \"tool\",\n                    LC.ACTION: \"missing_tool_method_decorator\",\n                    LC.CUSTOM: {\"function\": name},\n                },\n            ),\n        )\n        super().__init__(msg)\n</code></pre>"},{"location":"reference/core/Retry/","title":"Retry Policy","text":""},{"location":"reference/core/Retry/#graphorchestrator.core.retry","title":"<code>graphorchestrator.core.retry</code>","text":""},{"location":"reference/core/Retry/#graphorchestrator.core.retry.RetryPolicy","title":"<code>RetryPolicy</code>  <code>dataclass</code>","text":"<p>Defines a retry policy with configurable parameters.</p> <p>Attributes:</p> Name Type Description <code>max_retries</code> <code>int</code> <p>The maximum number of times to retry an operation.</p> <code>delay</code> <code>float</code> <p>The initial delay in seconds before the first retry.</p> <code>backoff</code> <code>float</code> <p>The factor by which to increase the delay after each retry.</p> Source code in <code>graphorchestrator\\core\\retry.py</code> <pre><code>@dataclass\nclass RetryPolicy:\n    \"\"\"\n    Defines a retry policy with configurable parameters.\n\n    Attributes:\n        max_retries (int): The maximum number of times to retry an operation.\n        delay (float): The initial delay in seconds before the first retry.\n        backoff (float): The factor by which to increase the delay after each retry.\n    \"\"\"\n\n    max_retries: int = 3\n    delay: float = 1.0\n    backoff: float = 2.0\n\n    def __str__(self) -&gt; str:\n        return f\"RetryPolicy(max_retries={self.max_retries}, delay={self.delay:.2f}s, backoff={self.backoff:.2f}x)\"\n\n    def __repr__(self) -&gt; str:\n        return str(self)  # same as __str__, unless you want a different debug style\n</code></pre>"},{"location":"reference/core/State/","title":"State","text":""},{"location":"reference/core/State/#graphorchestrator.core.state","title":"<code>graphorchestrator.core.state</code>","text":""},{"location":"reference/core/State/#graphorchestrator.core.state.State","title":"<code>State</code>  <code>dataclass</code>","text":"<p>Represents the state of a process or workflow, maintaining a list of messages.</p> <p>Attributes:</p> Name Type Description <code>messages</code> <code>List[Any]</code> <p>A list to store messages related to the state.</p> Source code in <code>graphorchestrator\\core\\state.py</code> <pre><code>@dataclass\nclass State:\n    \"\"\"\n    Represents the state of a process or workflow, maintaining a list of messages.\n\n    Attributes:\n        messages (List[Any]): A list to store messages related to the state.\n    \"\"\"\n\n    messages: List[Any] = field(default_factory=list)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Provides a string representation of the State object.\n\n        Returns:\n            str: A string representation of the State, including its messages.\n        \"\"\"\n        return f\"State({self.messages})\"\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"\n        Checks if the current State object is equal to another object.\n\n        Args:\n            other (Any): The object to compare with.\n\n        Returns:\n            bool: True if the objects are equal, False otherwise.\n        \"\"\"\n        if not isinstance(other, State):\n            return NotImplemented\n        return self.messages == other.messages\n</code></pre>"},{"location":"reference/core/State/#graphorchestrator.core.state.State.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Checks if the current State object is equal to another object.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Any</code> <p>The object to compare with.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the objects are equal, False otherwise.</p> Source code in <code>graphorchestrator\\core\\state.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"\n    Checks if the current State object is equal to another object.\n\n    Args:\n        other (Any): The object to compare with.\n\n    Returns:\n        bool: True if the objects are equal, False otherwise.\n    \"\"\"\n    if not isinstance(other, State):\n        return NotImplemented\n    return self.messages == other.messages\n</code></pre>"},{"location":"reference/core/State/#graphorchestrator.core.state.State.__repr__","title":"<code>__repr__()</code>","text":"<p>Provides a string representation of the State object.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the State, including its messages.</p> Source code in <code>graphorchestrator\\core\\state.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Provides a string representation of the State object.\n\n    Returns:\n        str: A string representation of the State, including its messages.\n    \"\"\"\n    return f\"State({self.messages})\"\n</code></pre>"},{"location":"reference/edges/Concrete/","title":"Concrete Edge","text":""},{"location":"reference/edges/Concrete/#graphorchestrator.edges.concrete","title":"<code>graphorchestrator.edges.concrete</code>","text":""},{"location":"reference/edges/Concrete/#graphorchestrator.edges.concrete.ConcreteEdge","title":"<code>ConcreteEdge</code>","text":"<p>               Bases: <code>Edge</code></p> <p>Concrete implementation of an edge in a graph.</p> <p>This class represents a direct, unconditional connection between a source node and a sink node in the graph. It logs the creation of the edge for debugging and monitoring purposes.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>Node</code> <p>The source node of the edge.</p> <code>sink</code> <code>Node</code> <p>The sink node of the edge.</p> Source code in <code>graphorchestrator\\edges\\concrete.py</code> <pre><code>class ConcreteEdge(Edge):\n    \"\"\"Concrete implementation of an edge in a graph.\n\n    This class represents a direct, unconditional connection between\n    a source node and a sink node in the graph.\n    It logs the creation of the edge for debugging and monitoring purposes.\n\n    Attributes:\n        source (Node): The source node of the edge.\n        sink (Node): The sink node of the edge.\n    \"\"\"\n\n    def __init__(self, source: Node, sink: Node):\n        \"\"\"Initializes a ConcreteEdge object.\n\n        This method sets up the connection between a source node and\n        a sink node, and logs the creation of this edge.\n\n        Args:\n            source (Node): The source node of the edge.\n            sink (Node): The sink node of the edge.\n        \"\"\"\n        self.source = source\n        # Sets the source node of the edge.\n        self.sink = sink\n        # Sets the sink node of the edge.\n\n        GraphLogger.get().info(\n            # Logs the creation of the edge with relevant details.\n            **wrap_constants(\n                # Wraps the log message with necessary constants.\n                message=\"Concrete edge created\",\n                **{\n                    # Additional details for the log message.\n                    LC.EVENT_TYPE: \"edge\",\n                    LC.ACTION: \"edge_created\",\n                    LC.EDGE_TYPE: \"concrete\",\n                    LC.SOURCE_NODE: self.source.node_id,\n                    LC.SINK_NODE: self.sink.node_id,\n                }\n            )\n        )\n</code></pre>"},{"location":"reference/edges/Concrete/#graphorchestrator.edges.concrete.ConcreteEdge.__init__","title":"<code>__init__(source, sink)</code>","text":"<p>Initializes a ConcreteEdge object.</p> <p>This method sets up the connection between a source node and a sink node, and logs the creation of this edge.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Node</code> <p>The source node of the edge.</p> required <code>sink</code> <code>Node</code> <p>The sink node of the edge.</p> required Source code in <code>graphorchestrator\\edges\\concrete.py</code> <pre><code>def __init__(self, source: Node, sink: Node):\n    \"\"\"Initializes a ConcreteEdge object.\n\n    This method sets up the connection between a source node and\n    a sink node, and logs the creation of this edge.\n\n    Args:\n        source (Node): The source node of the edge.\n        sink (Node): The sink node of the edge.\n    \"\"\"\n    self.source = source\n    # Sets the source node of the edge.\n    self.sink = sink\n    # Sets the sink node of the edge.\n\n    GraphLogger.get().info(\n        # Logs the creation of the edge with relevant details.\n        **wrap_constants(\n            # Wraps the log message with necessary constants.\n            message=\"Concrete edge created\",\n            **{\n                # Additional details for the log message.\n                LC.EVENT_TYPE: \"edge\",\n                LC.ACTION: \"edge_created\",\n                LC.EDGE_TYPE: \"concrete\",\n                LC.SOURCE_NODE: self.source.node_id,\n                LC.SINK_NODE: self.sink.node_id,\n            }\n        )\n    )\n</code></pre>"},{"location":"reference/edges/Conditional/","title":"Conditional Edge","text":""},{"location":"reference/edges/Conditional/#graphorchestrator.edges.conditional","title":"<code>graphorchestrator.edges.conditional</code>","text":""},{"location":"reference/edges/Conditional/#graphorchestrator.edges.conditional.ConditionalEdge","title":"<code>ConditionalEdge</code>","text":"<p>               Bases: <code>Edge</code></p> <p>Represents a conditional edge in a graph.</p> <p>A ConditionalEdge directs the flow of execution to one of several sink nodes based on the result of a routing function.</p> Source code in <code>graphorchestrator\\edges\\conditional.py</code> <pre><code>class ConditionalEdge(Edge):\n    \"\"\"\n    Represents a conditional edge in a graph.\n\n    A ConditionalEdge directs the flow of execution to one of several sink nodes\n    based on the result of a routing function.\n    \"\"\"\n\n    def __init__(\n        self, source: Node, sinks: List[Node], router: Callable[[State], str]\n    ) -&gt; None:\n        \"\"\"\n        Initializes a ConditionalEdge.\n\n        Args:\n            source (Node): The source node of the edge.\n            sinks (List[Node]): A list of sink nodes.\n            router (Callable[[State], str]): A routing function that takes a State object and returns\n                the ID of the sink node to which the edge should route.\n\n        Raises:\n            RoutingFunctionNotDecoratedError: If the router function is not decorated with\n                @routing_function.\n\n        \"\"\"\n\n        self.source = source\n        self.sinks = sinks\n\n        if not getattr(router, \"is_routing_function\", False):\n            raise RoutingFunctionNotDecoratedError(router)\n\n        self.routing_function = router\n        sink_ids = [s.node_id for s in sinks]\n\n        GraphLogger.get().info(\n            **wrap_constants(\n                message=\"Conditional edge created\",\n                **{\n                    LC.EVENT_TYPE: \"edge\",\n                    LC.ACTION: \"edge_created\",\n                    LC.EDGE_TYPE: \"conditional\",\n                    LC.SOURCE_NODE: self.source.node_id,\n                    LC.SINK_NODE: sink_ids,  # Using SINK_NODE for consistency; optional to split as LC.SINK_NODES\n                    LC.ROUTER_FUNC: router.__name__,\n                }\n            )\n        )\n</code></pre>"},{"location":"reference/edges/Conditional/#graphorchestrator.edges.conditional.ConditionalEdge.__init__","title":"<code>__init__(source, sinks, router)</code>","text":"<p>Initializes a ConditionalEdge.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Node</code> <p>The source node of the edge.</p> required <code>sinks</code> <code>List[Node]</code> <p>A list of sink nodes.</p> required <code>router</code> <code>Callable[[State], str]</code> <p>A routing function that takes a State object and returns the ID of the sink node to which the edge should route.</p> required <p>Raises:</p> Type Description <code>RoutingFunctionNotDecoratedError</code> <p>If the router function is not decorated with @routing_function.</p> Source code in <code>graphorchestrator\\edges\\conditional.py</code> <pre><code>def __init__(\n    self, source: Node, sinks: List[Node], router: Callable[[State], str]\n) -&gt; None:\n    \"\"\"\n    Initializes a ConditionalEdge.\n\n    Args:\n        source (Node): The source node of the edge.\n        sinks (List[Node]): A list of sink nodes.\n        router (Callable[[State], str]): A routing function that takes a State object and returns\n            the ID of the sink node to which the edge should route.\n\n    Raises:\n        RoutingFunctionNotDecoratedError: If the router function is not decorated with\n            @routing_function.\n\n    \"\"\"\n\n    self.source = source\n    self.sinks = sinks\n\n    if not getattr(router, \"is_routing_function\", False):\n        raise RoutingFunctionNotDecoratedError(router)\n\n    self.routing_function = router\n    sink_ids = [s.node_id for s in sinks]\n\n    GraphLogger.get().info(\n        **wrap_constants(\n            message=\"Conditional edge created\",\n            **{\n                LC.EVENT_TYPE: \"edge\",\n                LC.ACTION: \"edge_created\",\n                LC.EDGE_TYPE: \"conditional\",\n                LC.SOURCE_NODE: self.source.node_id,\n                LC.SINK_NODE: sink_ids,  # Using SINK_NODE for consistency; optional to split as LC.SINK_NODES\n                LC.ROUTER_FUNC: router.__name__,\n            }\n        )\n    )\n</code></pre>"},{"location":"reference/nodes/AIAction/","title":"AIAction","text":""},{"location":"reference/nodes/AIAction/#graphorchestrator.ai.ai_action","title":"<code>graphorchestrator.ai.ai_action</code>","text":""},{"location":"reference/nodes/AIAction/#graphorchestrator.ai.ai_action.AIActionBase","title":"<code>AIActionBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for defining AI actions within the graph orchestration framework.</p> <p>This class serves as a template for creating specialized nodes that can interact with and modify the state of a graph using an underlying AI model.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>dict</code> <p>A dictionary containing configuration parameters for the AI model.</p> <code>_model_built</code> <code>bool</code> <p>A flag indicating whether the AI model has been built.</p> <code>model</code> <code>Any</code> <p>The AI model instance.</p> <code>is_node_action</code> <code>bool</code> <p>A flag indicating that this object is an AI node action.</p> <code>__name__</code> <code>str</code> <p>The name of the AI action, which defaults to the class name.</p> Source code in <code>graphorchestrator\\ai\\ai_action.py</code> <pre><code>class AIActionBase(ABC):\n    \"\"\"Abstract base class for defining AI actions within the graph orchestration framework.\n\n    This class serves as a template for creating specialized nodes that can interact\n    with and modify the state of a graph using an underlying AI model.\n\n    Attributes:\n        config (dict): A dictionary containing configuration parameters for the AI model.\n        _model_built (bool): A flag indicating whether the AI model has been built.\n        model (Any): The AI model instance.\n        is_node_action (bool): A flag indicating that this object is an AI node action.\n        __name__ (str): The name of the AI action, which defaults to the class name.\n    \"\"\"\n\n    def __init__(self, config: dict) -&gt; None:\n        \"\"\"\n        Initializes an AIActionBase instance.\n\n        Args:\n            config (dict): Configuration parameters for the AI model.\n        \"\"\"\n        # Store the provided configuration\n        self.config: dict = config\n        # Initialize flags and model to default states\n        self._model_built = False\n        self.model: Any = None\n        # Identify the instance as an AI node action\n        self.is_node_action = True\n        # Set the name to the class name\n        self.__name__ = self.__class__.__name__\n\n        # Get the logger instance\n        log = GraphLogger.get()\n        # Use the class name as the node label\n        node_label = self.__name__\n\n        # Log the initialization of the AIActionBase instance\n        log.info(\n            **wrap_constants(\n                message=\"AIActionBase initialized\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.ACTION: \"node_created\",  # Action being taken: node creation\n                    LC.NODE_ID: node_label,\n                    LC.NODE_TYPE: \"AINode\",\n                    LC.CUSTOM: {\"config\": self.config},  # Full config included here\n                }\n            )\n        )\n\n    @abstractmethod\n    def build_model(self) -&gt; None:\n        \"\"\"Build and configure the AI model.\n\n        This method should be implemented by subclasses to handle the creation and\n        configuration of the AI model. It is typically called before processing any state.\n\n        Subclasses must set the following attributes:\n            self.model: The constructed AI model instance.\n            self._model_built: Set to True to indicate the model is built.\n\n        Raises:\n            NotImplementedError: If the method is not implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def process_state(self, state: State) -&gt; State:\n        \"\"\"Process the state using the AI model.\n\n        This is the main method where the AI model logic is applied to the current state.\n        Subclasses must implement this method to define how the AI model modifies the state.\n\n        Args:\n            state (State): The current state to be processed.\n\n        Returns:\n            State: The new state after processing.\n        \"\"\"\n        raise NotImplementedError\n\n    async def __call__(self, state: State) -&gt; State:\n        \"\"\"\n        Invokes the AI action's processing logic.\n        \"\"\"\n        # Get the logger instance and the node label\n        log = GraphLogger.get()\n        node_label = getattr(self, \"__name__\", self.__class__.__name__)\n\n        # If the model has not been built yet, build it\n        if not self._model_built:\n            self.build_model()\n\n            # Log that the AI model has been built\n            log.info(\n                **wrap_constants(\n                    message=\"AI model built\",\n                    **{\n                        LC.EVENT_TYPE: \"node\",\n                        LC.NODE_ID: node_label,\n                        LC.NODE_TYPE: \"AINode\",\n                        LC.ACTION: \"build_model\",\n                        LC.CUSTOM: {\"config\": self.config},\n                    }\n                )\n            )\n\n        # Log the start of the AI node execution\n        log.info(\n            **wrap_constants(\n                message=\"AI node execution started\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: node_label,\n                    LC.NODE_TYPE: \"AINode\",\n                    LC.ACTION: \"execute_start\",\n                    LC.INPUT_SIZE: len(state.messages),\n                }\n            )\n        )\n\n        # Process the state using the AI model\n        result_or_coro = self.process_state(state)\n        # Handle coroutines if needed\n        result = (\n            await result_or_coro\n            if isinstance(result_or_coro, abc.Awaitable)\n            else result_or_coro\n        )\n\n        # Validate the output\n        # Ensure that the output is a State object; if not, raise an error\n        if not isinstance(result, State):\n            log.error(\n                **wrap_constants(\n                    message=\"AI action returned non-State object\",\n                    **{\n                        LC.EVENT_TYPE: \"node\",\n                        LC.NODE_ID: node_label,\n                        LC.NODE_TYPE: \"AINode\",\n                        LC.ACTION: \"invalid_output\",\n                        LC.CUSTOM: {\"actual_type\": str(type(result))},\n                    }\n                )\n            )\n            raise InvalidAIActionOutput(result)\n\n        # Log the completion of the AI node execution\n        log.info(\n            **wrap_constants(\n                message=\"AI node execution completed\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: node_label,\n                    LC.NODE_TYPE: \"AINode\",\n                    LC.ACTION: \"execute_end\",\n                    LC.OUTPUT_SIZE: len(result.messages),\n                    LC.SUCCESS: True,\n                }\n            )\n        )\n\n        # Return the processed state\n        return result\n</code></pre>"},{"location":"reference/nodes/AIAction/#graphorchestrator.ai.ai_action.AIActionBase.__call__","title":"<code>__call__(state)</code>  <code>async</code>","text":"<p>Invokes the AI action's processing logic.</p> Source code in <code>graphorchestrator\\ai\\ai_action.py</code> <pre><code>async def __call__(self, state: State) -&gt; State:\n    \"\"\"\n    Invokes the AI action's processing logic.\n    \"\"\"\n    # Get the logger instance and the node label\n    log = GraphLogger.get()\n    node_label = getattr(self, \"__name__\", self.__class__.__name__)\n\n    # If the model has not been built yet, build it\n    if not self._model_built:\n        self.build_model()\n\n        # Log that the AI model has been built\n        log.info(\n            **wrap_constants(\n                message=\"AI model built\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: node_label,\n                    LC.NODE_TYPE: \"AINode\",\n                    LC.ACTION: \"build_model\",\n                    LC.CUSTOM: {\"config\": self.config},\n                }\n            )\n        )\n\n    # Log the start of the AI node execution\n    log.info(\n        **wrap_constants(\n            message=\"AI node execution started\",\n            **{\n                LC.EVENT_TYPE: \"node\",\n                LC.NODE_ID: node_label,\n                LC.NODE_TYPE: \"AINode\",\n                LC.ACTION: \"execute_start\",\n                LC.INPUT_SIZE: len(state.messages),\n            }\n        )\n    )\n\n    # Process the state using the AI model\n    result_or_coro = self.process_state(state)\n    # Handle coroutines if needed\n    result = (\n        await result_or_coro\n        if isinstance(result_or_coro, abc.Awaitable)\n        else result_or_coro\n    )\n\n    # Validate the output\n    # Ensure that the output is a State object; if not, raise an error\n    if not isinstance(result, State):\n        log.error(\n            **wrap_constants(\n                message=\"AI action returned non-State object\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: node_label,\n                    LC.NODE_TYPE: \"AINode\",\n                    LC.ACTION: \"invalid_output\",\n                    LC.CUSTOM: {\"actual_type\": str(type(result))},\n                }\n            )\n        )\n        raise InvalidAIActionOutput(result)\n\n    # Log the completion of the AI node execution\n    log.info(\n        **wrap_constants(\n            message=\"AI node execution completed\",\n            **{\n                LC.EVENT_TYPE: \"node\",\n                LC.NODE_ID: node_label,\n                LC.NODE_TYPE: \"AINode\",\n                LC.ACTION: \"execute_end\",\n                LC.OUTPUT_SIZE: len(result.messages),\n                LC.SUCCESS: True,\n            }\n        )\n    )\n\n    # Return the processed state\n    return result\n</code></pre>"},{"location":"reference/nodes/AIAction/#graphorchestrator.ai.ai_action.AIActionBase.__init__","title":"<code>__init__(config)</code>","text":"<p>Initializes an AIActionBase instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration parameters for the AI model.</p> required Source code in <code>graphorchestrator\\ai\\ai_action.py</code> <pre><code>def __init__(self, config: dict) -&gt; None:\n    \"\"\"\n    Initializes an AIActionBase instance.\n\n    Args:\n        config (dict): Configuration parameters for the AI model.\n    \"\"\"\n    # Store the provided configuration\n    self.config: dict = config\n    # Initialize flags and model to default states\n    self._model_built = False\n    self.model: Any = None\n    # Identify the instance as an AI node action\n    self.is_node_action = True\n    # Set the name to the class name\n    self.__name__ = self.__class__.__name__\n\n    # Get the logger instance\n    log = GraphLogger.get()\n    # Use the class name as the node label\n    node_label = self.__name__\n\n    # Log the initialization of the AIActionBase instance\n    log.info(\n        **wrap_constants(\n            message=\"AIActionBase initialized\",\n            **{\n                LC.EVENT_TYPE: \"node\",\n                LC.ACTION: \"node_created\",  # Action being taken: node creation\n                LC.NODE_ID: node_label,\n                LC.NODE_TYPE: \"AINode\",\n                LC.CUSTOM: {\"config\": self.config},  # Full config included here\n            }\n        )\n    )\n</code></pre>"},{"location":"reference/nodes/AIAction/#graphorchestrator.ai.ai_action.AIActionBase.build_model","title":"<code>build_model()</code>  <code>abstractmethod</code>","text":"<p>Build and configure the AI model.</p> <p>This method should be implemented by subclasses to handle the creation and configuration of the AI model. It is typically called before processing any state.</p> Subclasses must set the following attributes <p>self.model: The constructed AI model instance. self._model_built: Set to True to indicate the model is built.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented by a subclass.</p> Source code in <code>graphorchestrator\\ai\\ai_action.py</code> <pre><code>@abstractmethod\ndef build_model(self) -&gt; None:\n    \"\"\"Build and configure the AI model.\n\n    This method should be implemented by subclasses to handle the creation and\n    configuration of the AI model. It is typically called before processing any state.\n\n    Subclasses must set the following attributes:\n        self.model: The constructed AI model instance.\n        self._model_built: Set to True to indicate the model is built.\n\n    Raises:\n        NotImplementedError: If the method is not implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nodes/AIAction/#graphorchestrator.ai.ai_action.AIActionBase.process_state","title":"<code>process_state(state)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Process the state using the AI model.</p> <p>This is the main method where the AI model logic is applied to the current state. Subclasses must implement this method to define how the AI model modifies the state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state to be processed.</p> required <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>The new state after processing.</p> Source code in <code>graphorchestrator\\ai\\ai_action.py</code> <pre><code>@abstractmethod\nasync def process_state(self, state: State) -&gt; State:\n    \"\"\"Process the state using the AI model.\n\n    This is the main method where the AI model logic is applied to the current state.\n    Subclasses must implement this method to define how the AI model modifies the state.\n\n    Args:\n        state (State): The current state to be processed.\n\n    Returns:\n        State: The new state after processing.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nodes/AINode/","title":"Overview","text":""},{"location":"reference/nodes/AINode/#graphorchestrator.nodes.nodes.AINode","title":"<code>graphorchestrator.nodes.nodes.AINode</code>","text":"<p>               Bases: <code>ProcessingNode</code></p> <p>A node that represents an AI model.</p> <p>This node wraps an AI model action.</p> Source code in <code>graphorchestrator\\nodes\\nodes.py</code> <pre><code>class AINode(ProcessingNode):\n    \"\"\"\n    A node that represents an AI model.\n\n    This node wraps an AI model action.\n    \"\"\"\n\n    def __init__(\n        self,\n        node_id: str,\n        description: str,\n        model_action: Callable[[State], State],\n        response_format: Optional[str] = None,\n        response_parser: Optional[Callable[[State], Any]] = None,\n    ) -&gt; None:\n        super().__init__(node_id, model_action)\n        self.description = description\n        self.response_format = response_format\n        self.response_parser = response_parser\n\n        GraphLogger.get().info(\n            **wrap_constants(\n                message=\"AINode created\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: self.node_id,\n                    LC.NODE_TYPE: \"AINode\",\n                    LC.ACTION: \"node_created\",\n                    LC.CUSTOM: {\n                        \"description\": description,\n                        \"response_format\": response_format,\n                        \"has_parser\": bool(response_parser),\n                    },\n                },\n            )\n        )\n\n    async def execute(self, state: State) -&gt; State:\n        \"\"\"\n        Executes the AI model action.\n\n        Args:\n            state (State): The input state for the node.\n\n        Returns:\n            State: The state after executing the model action.\n        \"\"\"\n        log = GraphLogger.get()\n\n        log.info(\n            **wrap_constants(\n                message=\"AINode execution started\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: self.node_id,\n                    LC.NODE_TYPE: \"AINode\",\n                    LC.ACTION: \"execute_start\",\n                    LC.INPUT_SIZE: len(state.messages),\n                },\n            )\n        )\n\n        result = await self.func(state)\n\n        if not isinstance(result, State):\n            log.error(\n                **wrap_constants(\n                    message=\"AINode returned invalid output\",\n                    **{\n                        LC.EVENT_TYPE: \"node\",\n                        LC.NODE_ID: self.node_id,\n                        LC.NODE_TYPE: \"AINode\",\n                        LC.ACTION: \"invalid_output\",\n                        LC.CUSTOM: {\"result_type\": str(type(result))},\n                    },\n                )\n            )\n            raise InvalidAIActionOutput(result)\n\n        log.info(\n            **wrap_constants(\n                message=\"AINode execution completed\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: self.node_id,\n                    LC.NODE_TYPE: \"AINode\",\n                    LC.ACTION: \"execute_end\",\n                    LC.OUTPUT_SIZE: len(result.messages),\n                    LC.SUCCESS: True,\n                },\n            )\n        )\n\n        return result\n</code></pre>"},{"location":"reference/nodes/AINode/#graphorchestrator.nodes.nodes.AINode.execute","title":"<code>execute(state)</code>  <code>async</code>","text":"<p>Executes the AI model action.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The input state for the node.</p> required <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>The state after executing the model action.</p> Source code in <code>graphorchestrator\\nodes\\nodes.py</code> <pre><code>async def execute(self, state: State) -&gt; State:\n    \"\"\"\n    Executes the AI model action.\n\n    Args:\n        state (State): The input state for the node.\n\n    Returns:\n        State: The state after executing the model action.\n    \"\"\"\n    log = GraphLogger.get()\n\n    log.info(\n        **wrap_constants(\n            message=\"AINode execution started\",\n            **{\n                LC.EVENT_TYPE: \"node\",\n                LC.NODE_ID: self.node_id,\n                LC.NODE_TYPE: \"AINode\",\n                LC.ACTION: \"execute_start\",\n                LC.INPUT_SIZE: len(state.messages),\n            },\n        )\n    )\n\n    result = await self.func(state)\n\n    if not isinstance(result, State):\n        log.error(\n            **wrap_constants(\n                message=\"AINode returned invalid output\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: self.node_id,\n                    LC.NODE_TYPE: \"AINode\",\n                    LC.ACTION: \"invalid_output\",\n                    LC.CUSTOM: {\"result_type\": str(type(result))},\n                },\n            )\n        )\n        raise InvalidAIActionOutput(result)\n\n    log.info(\n        **wrap_constants(\n            message=\"AINode execution completed\",\n            **{\n                LC.EVENT_TYPE: \"node\",\n                LC.NODE_ID: self.node_id,\n                LC.NODE_TYPE: \"AINode\",\n                LC.ACTION: \"execute_end\",\n                LC.OUTPUT_SIZE: len(result.messages),\n                LC.SUCCESS: True,\n            },\n        )\n    )\n\n    return result\n</code></pre>"},{"location":"reference/nodes/AggregatorNode/","title":"Aggregator Node","text":""},{"location":"reference/nodes/AggregatorNode/#graphorchestrator.nodes.nodes.AggregatorNode","title":"<code>graphorchestrator.nodes.nodes.AggregatorNode</code>","text":"<p>               Bases: <code>Node</code></p> <p>A node that aggregates multiple states into a single state.</p> <p>This node takes a list of State objects, aggregates them, and returns a new State object representing the aggregated result.</p> Source code in <code>graphorchestrator\\nodes\\nodes.py</code> <pre><code>class AggregatorNode(Node):\n    \"\"\"\n    A node that aggregates multiple states into a single state.\n\n    This node takes a list of State objects, aggregates them, and returns a\n    new State object representing the aggregated result.\n    \"\"\"\n\n    def __init__(\n        self, node_id: str, aggregator_action: Callable[[List[State]], State]\n    ) -&gt; None:\n        super().__init__(node_id)\n        self.aggregator_action = aggregator_action\n        if not getattr(aggregator_action, \"is_aggregator_action\", False):\n            raise AggregatorActionNotDecorated(aggregator_action)\n\n        GraphLogger.get().info(\n            **wrap_constants(\n                message=\"AggregatorNode created\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: self.node_id,\n                    LC.NODE_TYPE: \"AggregatorNode\",\n                    LC.ACTION: \"node_created\",\n                    LC.CUSTOM: {\"function\": aggregator_action.__name__},\n                },\n            )\n        )\n\n    async def execute(self, states: List[State]) -&gt; State:\n        \"\"\"\n        Executes the aggregation logic of the node.\n\n        Args:\n            states (List[State]): The list of states to aggregate.\n\n        Returns:\n            State: The aggregated state.\n        \"\"\"\n        log = GraphLogger.get()\n\n        log.info(\n            **wrap_constants(\n                message=\"AggregatorNode execution started\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: self.node_id,\n                    LC.NODE_TYPE: \"AggregatorNode\",\n                    LC.ACTION: \"execute_start\",\n                    LC.CUSTOM: {\"input_batch_size\": len(states)},\n                },\n            )\n        )\n\n        result = (\n            await self.aggregator_action(states)\n            if asyncio.iscoroutinefunction(self.aggregator_action)\n            else self.aggregator_action(states)\n        )\n\n        log.info(\n            **wrap_constants(\n                message=\"AggregatorNode execution completed\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: self.node_id,\n                    LC.NODE_TYPE: \"AggregatorNode\",\n                    LC.ACTION: \"execute_end\",\n                    LC.OUTPUT_SIZE: len(result.messages),\n                },\n            )\n        )\n\n        return result\n</code></pre>"},{"location":"reference/nodes/AggregatorNode/#graphorchestrator.nodes.nodes.AggregatorNode.execute","title":"<code>execute(states)</code>  <code>async</code>","text":"<p>Executes the aggregation logic of the node.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>List[State]</code> <p>The list of states to aggregate.</p> required <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>The aggregated state.</p> Source code in <code>graphorchestrator\\nodes\\nodes.py</code> <pre><code>async def execute(self, states: List[State]) -&gt; State:\n    \"\"\"\n    Executes the aggregation logic of the node.\n\n    Args:\n        states (List[State]): The list of states to aggregate.\n\n    Returns:\n        State: The aggregated state.\n    \"\"\"\n    log = GraphLogger.get()\n\n    log.info(\n        **wrap_constants(\n            message=\"AggregatorNode execution started\",\n            **{\n                LC.EVENT_TYPE: \"node\",\n                LC.NODE_ID: self.node_id,\n                LC.NODE_TYPE: \"AggregatorNode\",\n                LC.ACTION: \"execute_start\",\n                LC.CUSTOM: {\"input_batch_size\": len(states)},\n            },\n        )\n    )\n\n    result = (\n        await self.aggregator_action(states)\n        if asyncio.iscoroutinefunction(self.aggregator_action)\n        else self.aggregator_action(states)\n    )\n\n    log.info(\n        **wrap_constants(\n            message=\"AggregatorNode execution completed\",\n            **{\n                LC.EVENT_TYPE: \"node\",\n                LC.NODE_ID: self.node_id,\n                LC.NODE_TYPE: \"AggregatorNode\",\n                LC.ACTION: \"execute_end\",\n                LC.OUTPUT_SIZE: len(result.messages),\n            },\n        )\n    )\n\n    return result\n</code></pre>"},{"location":"reference/nodes/Base/","title":"Base Node","text":""},{"location":"reference/nodes/Base/#graphorchestrator.nodes.base","title":"<code>graphorchestrator.nodes.base</code>","text":""},{"location":"reference/nodes/Base/#graphorchestrator.nodes.base.Node","title":"<code>Node</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class representing a node in a graph.</p> <p>Nodes have unique IDs and can have incoming and outgoing edges.</p> Source code in <code>graphorchestrator\\nodes\\base.py</code> <pre><code>class Node(ABC):\n    \"\"\"\n    Abstract base class representing a node in a graph.\n\n    Nodes have unique IDs and can have incoming and outgoing edges.\n    \"\"\"\n\n    def __init__(self, node_id: str) -&gt; None:\n        \"\"\"\n        Initializes a new Node instance.\n\n        Args:\n            node_id (str): The unique identifier for this node.\n        \"\"\"\n        self.node_id: str = node_id\n        self.incoming_edges = []\n        self.outgoing_edges = []\n        self.fallback_node_id: Optional[str] = None\n        self.retry_policy: Optional[RetryPolicy] = None\n\n        GraphLogger.get().info(\n            **wrap_constants(\n                message=\"Node initialized\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: self.node_id,\n                    LC.NODE_TYPE: self.__class__.__name__,\n                    LC.ACTION: \"node_created\",\n                    LC.CUSTOM: {\n                        \"incoming_edges\": 0,\n                        \"outgoing_edges\": 0,\n                        \"fallback_node_id\": None,\n                        \"retry_policy\": None,\n                    },\n                }\n            )\n        )\n\n    @abstractmethod\n    def execute(self, state: State):\n        \"\"\"\n        Abstract method to execute the node's logic.\n\n        Args:\n            state: The current state of the execution.\n        \"\"\"\n        raise NotImplementedError\n\n    def set_fallback(self, fallback_node_id: str) -&gt; None:\n        \"\"\"\n        Sets the fallback node ID for this node.\n\n        Args:\n            fallback_node_id (str): The ID of the fallback node.\n        \"\"\"\n        self.fallback_node_id = fallback_node_id\n\n    def set_retry_policy(self, retry_policy: RetryPolicy) -&gt; None:\n        \"\"\"\n        Sets the retry policy for this node.\n\n        Args:\n            retry_policy (RetryPolicy): The retry policy to apply.\n        \"\"\"\n        self.retry_policy = retry_policy\n</code></pre>"},{"location":"reference/nodes/Base/#graphorchestrator.nodes.base.Node.__init__","title":"<code>__init__(node_id)</code>","text":"<p>Initializes a new Node instance.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The unique identifier for this node.</p> required Source code in <code>graphorchestrator\\nodes\\base.py</code> <pre><code>def __init__(self, node_id: str) -&gt; None:\n    \"\"\"\n    Initializes a new Node instance.\n\n    Args:\n        node_id (str): The unique identifier for this node.\n    \"\"\"\n    self.node_id: str = node_id\n    self.incoming_edges = []\n    self.outgoing_edges = []\n    self.fallback_node_id: Optional[str] = None\n    self.retry_policy: Optional[RetryPolicy] = None\n\n    GraphLogger.get().info(\n        **wrap_constants(\n            message=\"Node initialized\",\n            **{\n                LC.EVENT_TYPE: \"node\",\n                LC.NODE_ID: self.node_id,\n                LC.NODE_TYPE: self.__class__.__name__,\n                LC.ACTION: \"node_created\",\n                LC.CUSTOM: {\n                    \"incoming_edges\": 0,\n                    \"outgoing_edges\": 0,\n                    \"fallback_node_id\": None,\n                    \"retry_policy\": None,\n                },\n            }\n        )\n    )\n</code></pre>"},{"location":"reference/nodes/Base/#graphorchestrator.nodes.base.Node.execute","title":"<code>execute(state)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to execute the node's logic.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current state of the execution.</p> required Source code in <code>graphorchestrator\\nodes\\base.py</code> <pre><code>@abstractmethod\ndef execute(self, state: State):\n    \"\"\"\n    Abstract method to execute the node's logic.\n\n    Args:\n        state: The current state of the execution.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nodes/Base/#graphorchestrator.nodes.base.Node.set_fallback","title":"<code>set_fallback(fallback_node_id)</code>","text":"<p>Sets the fallback node ID for this node.</p> <p>Parameters:</p> Name Type Description Default <code>fallback_node_id</code> <code>str</code> <p>The ID of the fallback node.</p> required Source code in <code>graphorchestrator\\nodes\\base.py</code> <pre><code>def set_fallback(self, fallback_node_id: str) -&gt; None:\n    \"\"\"\n    Sets the fallback node ID for this node.\n\n    Args:\n        fallback_node_id (str): The ID of the fallback node.\n    \"\"\"\n    self.fallback_node_id = fallback_node_id\n</code></pre>"},{"location":"reference/nodes/Base/#graphorchestrator.nodes.base.Node.set_retry_policy","title":"<code>set_retry_policy(retry_policy)</code>","text":"<p>Sets the retry policy for this node.</p> <p>Parameters:</p> Name Type Description Default <code>retry_policy</code> <code>RetryPolicy</code> <p>The retry policy to apply.</p> required Source code in <code>graphorchestrator\\nodes\\base.py</code> <pre><code>def set_retry_policy(self, retry_policy: RetryPolicy) -&gt; None:\n    \"\"\"\n    Sets the retry policy for this node.\n\n    Args:\n        retry_policy (RetryPolicy): The retry policy to apply.\n    \"\"\"\n    self.retry_policy = retry_policy\n</code></pre>"},{"location":"reference/nodes/HITLNode/","title":"Human In the Loop (HITL) Node","text":""},{"location":"reference/nodes/HITLNode/#graphorchestrator.nodes.nodes.HumanInTheLoopNode","title":"<code>graphorchestrator.nodes.nodes.HumanInTheLoopNode</code>","text":"<p>               Bases: <code>ProcessingNode</code></p> <p>A node that pauses execution for human input.</p> Source code in <code>graphorchestrator\\nodes\\nodes.py</code> <pre><code>class HumanInTheLoopNode(ProcessingNode):\n    \"\"\"\n    A node that pauses execution for human input.\n    \"\"\"\n\n    def __init__(\n        self,\n        node_id: str,\n        interaction_handler: Callable[[State], Awaitable[State]],\n        metadata: Optional[Dict[str, str]] = None,\n    ) -&gt; None:\n        if not getattr(interaction_handler, \"is_node_action\", False):\n            interaction_handler = node_action(interaction_handler)\n\n        self.metadata = metadata or {}\n\n        GraphLogger.get().info(\n            **wrap_constants(\n                message=\"Human-in-the-loop node created\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: node_id,\n                    LC.NODE_TYPE: \"HumanInTheLoopNode\",\n                    LC.ACTION: \"node_created\",\n                    LC.CUSTOM: {\"metadata_keys\": list(self.metadata.keys())},\n                },\n            )\n        )\n\n        super().__init__(node_id, interaction_handler)\n\n    async def execute(self, state: State) -&gt; State:\n        \"\"\"\n        Executes the human-in-the-loop interaction.\n        \"\"\"\n        log = GraphLogger.get()\n\n        log.info(\n            **wrap_constants(\n                message=\"HumanInTheLoopNode execution started\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: self.node_id,\n                    LC.NODE_TYPE: \"HumanInTheLoopNode\",\n                    LC.ACTION: \"execute_start\",\n                    LC.INPUT_SIZE: len(state.messages),\n                },\n            )\n        )\n\n        result = await self.func(state)\n\n        if not isinstance(result, State):\n            log.error(\n                **wrap_constants(\n                    message=\"Invalid output from human-in-the-loop handler\",\n                    **{\n                        LC.EVENT_TYPE: \"node\",\n                        LC.NODE_ID: self.node_id,\n                        LC.NODE_TYPE: \"HumanInTheLoopNode\",\n                        LC.ACTION: \"invalid_output\",\n                        LC.CUSTOM: {\"result_type\": str(type(result))},\n                    },\n                )\n            )\n            raise InvalidNodeActionOutput(result)\n\n        log.info(\n            **wrap_constants(\n                message=\"HumanInTheLoopNode execution completed\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: self.node_id,\n                    LC.NODE_TYPE: \"HumanInTheLoopNode\",\n                    LC.ACTION: \"execute_end\",\n                    LC.OUTPUT_SIZE: len(result.messages),\n                    LC.SUCCESS: True,\n                },\n            )\n        )\n\n        return result\n</code></pre>"},{"location":"reference/nodes/HITLNode/#graphorchestrator.nodes.nodes.HumanInTheLoopNode.execute","title":"<code>execute(state)</code>  <code>async</code>","text":"<p>Executes the human-in-the-loop interaction.</p> Source code in <code>graphorchestrator\\nodes\\nodes.py</code> <pre><code>async def execute(self, state: State) -&gt; State:\n    \"\"\"\n    Executes the human-in-the-loop interaction.\n    \"\"\"\n    log = GraphLogger.get()\n\n    log.info(\n        **wrap_constants(\n            message=\"HumanInTheLoopNode execution started\",\n            **{\n                LC.EVENT_TYPE: \"node\",\n                LC.NODE_ID: self.node_id,\n                LC.NODE_TYPE: \"HumanInTheLoopNode\",\n                LC.ACTION: \"execute_start\",\n                LC.INPUT_SIZE: len(state.messages),\n            },\n        )\n    )\n\n    result = await self.func(state)\n\n    if not isinstance(result, State):\n        log.error(\n            **wrap_constants(\n                message=\"Invalid output from human-in-the-loop handler\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: self.node_id,\n                    LC.NODE_TYPE: \"HumanInTheLoopNode\",\n                    LC.ACTION: \"invalid_output\",\n                    LC.CUSTOM: {\"result_type\": str(type(result))},\n                },\n            )\n        )\n        raise InvalidNodeActionOutput(result)\n\n    log.info(\n        **wrap_constants(\n            message=\"HumanInTheLoopNode execution completed\",\n            **{\n                LC.EVENT_TYPE: \"node\",\n                LC.NODE_ID: self.node_id,\n                LC.NODE_TYPE: \"HumanInTheLoopNode\",\n                LC.ACTION: \"execute_end\",\n                LC.OUTPUT_SIZE: len(result.messages),\n                LC.SUCCESS: True,\n            },\n        )\n    )\n\n    return result\n</code></pre>"},{"location":"reference/nodes/ProcessingNode/","title":"Processing Node","text":""},{"location":"reference/nodes/ProcessingNode/#graphorchestrator.nodes.nodes.ProcessingNode","title":"<code>graphorchestrator.nodes.nodes.ProcessingNode</code>","text":"<p>               Bases: <code>Node</code></p> <p>A node that processes the state.</p> <p>This node takes a function that operates on a State object, processes it, and returns a modified State object.</p> Source code in <code>graphorchestrator\\nodes\\nodes.py</code> <pre><code>class ProcessingNode(Node):\n    \"\"\"\n    A node that processes the state.\n\n    This node takes a function that operates on a State object, processes it,\n    and returns a modified State object.\n    \"\"\"\n\n    def __init__(self, node_id: str, func: Callable[[State], State]) -&gt; None:\n        super().__init__(node_id)\n        self.func = func\n        if not getattr(func, \"is_node_action\", False):\n            raise NodeActionNotDecoratedError(func)\n\n        GraphLogger.get().info(\n            **wrap_constants(\n                message=\"ProcessingNode created\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: self.node_id,\n                    LC.NODE_TYPE: \"ProcessingNode\",\n                    LC.ACTION: \"node_created\",\n                    LC.CUSTOM: {\"function\": func.__name__},\n                },\n            )\n        )\n\n    async def execute(self, state: State) -&gt; State:\n        \"\"\"\n        Executes the processing logic of the node.\n\n        Args:\n            state (State): The input state for the node.\n\n        Returns:\n            State: The modified state after processing.\n        \"\"\"\n        log = GraphLogger.get()\n\n        log.info(\n            **wrap_constants(\n                message=\"ProcessingNode execution started\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: self.node_id,\n                    LC.NODE_TYPE: \"ProcessingNode\",\n                    LC.ACTION: \"execute_start\",\n                    LC.INPUT_SIZE: len(state.messages),\n                },\n            )\n        )\n\n        result = (\n            await self.func(state)\n            if asyncio.iscoroutinefunction(self.func)\n            else self.func(state)\n        )\n\n        log.info(\n            **wrap_constants(\n                message=\"ProcessingNode execution completed\",\n                **{\n                    LC.EVENT_TYPE: \"node\",\n                    LC.NODE_ID: self.node_id,\n                    LC.NODE_TYPE: \"ProcessingNode\",\n                    LC.ACTION: \"execute_end\",\n                    LC.OUTPUT_SIZE: len(result.messages),\n                },\n            )\n        )\n\n        return result\n</code></pre>"},{"location":"reference/nodes/ProcessingNode/#graphorchestrator.nodes.nodes.ProcessingNode.execute","title":"<code>execute(state)</code>  <code>async</code>","text":"<p>Executes the processing logic of the node.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The input state for the node.</p> required <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>The modified state after processing.</p> Source code in <code>graphorchestrator\\nodes\\nodes.py</code> <pre><code>async def execute(self, state: State) -&gt; State:\n    \"\"\"\n    Executes the processing logic of the node.\n\n    Args:\n        state (State): The input state for the node.\n\n    Returns:\n        State: The modified state after processing.\n    \"\"\"\n    log = GraphLogger.get()\n\n    log.info(\n        **wrap_constants(\n            message=\"ProcessingNode execution started\",\n            **{\n                LC.EVENT_TYPE: \"node\",\n                LC.NODE_ID: self.node_id,\n                LC.NODE_TYPE: \"ProcessingNode\",\n                LC.ACTION: \"execute_start\",\n                LC.INPUT_SIZE: len(state.messages),\n            },\n        )\n    )\n\n    result = (\n        await self.func(state)\n        if asyncio.iscoroutinefunction(self.func)\n        else self.func(state)\n    )\n\n    log.info(\n        **wrap_constants(\n            message=\"ProcessingNode execution completed\",\n            **{\n                LC.EVENT_TYPE: \"node\",\n                LC.NODE_ID: self.node_id,\n                LC.NODE_TYPE: \"ProcessingNode\",\n                LC.ACTION: \"execute_end\",\n                LC.OUTPUT_SIZE: len(result.messages),\n            },\n        )\n    )\n\n    return result\n</code></pre>"},{"location":"reference/nodes/ToolNode/","title":"Tool Node","text":""},{"location":"reference/nodes/ToolNode/#graphorchestrator.nodes.nodes.ToolNode","title":"<code>graphorchestrator.nodes.nodes.ToolNode</code>","text":"<p>               Bases: <code>ProcessingNode</code></p> <p>A node that represents a tool.</p> <p>This node is a specialized ProcessingNode that wraps a tool method.</p> Source code in <code>graphorchestrator\\nodes\\nodes.py</code> <pre><code>class ToolNode(ProcessingNode):\n    \"\"\"\n    A node that represents a tool.\n\n    This node is a specialized ProcessingNode that wraps a tool method.\n    \"\"\"\n\n    def __init__(\n        self,\n        node_id: str,\n        description: Optional[str],\n        tool_method: Callable[[State], State],\n    ) -&gt; None:\n        if not getattr(tool_method, \"is_tool_method\", False):\n            raise ToolMethodNotDecorated(tool_method)\n        if not (description or (tool_method.__doc__ or \"\").strip()):\n            raise EmptyToolNodeDescriptionError(tool_method)\n\n        super().__init__(node_id, tool_method)\n        self.description = description\n\n        GraphLogger.get().info(\n            **wrap_constants(\n                message=\"ToolNode created\",\n                **{\n                    LC.EVENT_TYPE: \"tool\",\n                    LC.NODE_ID: self.node_id,\n                    LC.NODE_TYPE: \"ToolNode\",\n                    LC.ACTION: \"node_created\",\n                    LC.CUSTOM: {\n                        \"function\": tool_method.__name__,\n                        \"has_description\": bool(description),\n                    },\n                },\n            )\n        )\n\n    async def execute(self, state: State) -&gt; State:\n        \"\"\"\n        Executes the tool method.\n\n        Args:\n            state (State): The input state for the node.\n\n        Returns:\n            State: The state after executing the tool method.\n        \"\"\"\n        log = GraphLogger.get()\n\n        log.info(\n            **wrap_constants(\n                message=\"ToolNode execution started\",\n                **{\n                    LC.EVENT_TYPE: \"tool\",\n                    LC.NODE_ID: self.node_id,\n                    LC.NODE_TYPE: \"ToolNode\",\n                    LC.ACTION: \"execute_start\",\n                    LC.INPUT_SIZE: len(state.messages),\n                },\n            )\n        )\n\n        result = (\n            await self.func(state)\n            if asyncio.iscoroutinefunction(self.func)\n            else self.func(state)\n        )\n\n        log.info(\n            **wrap_constants(\n                message=\"ToolNode execution completed\",\n                **{\n                    LC.EVENT_TYPE: \"tool\",\n                    LC.NODE_ID: self.node_id,\n                    LC.NODE_TYPE: \"ToolNode\",\n                    LC.ACTION: \"execute_end\",\n                    LC.OUTPUT_SIZE: len(result.messages),\n                },\n            )\n        )\n\n        return result\n</code></pre>"},{"location":"reference/nodes/ToolNode/#graphorchestrator.nodes.nodes.ToolNode.execute","title":"<code>execute(state)</code>  <code>async</code>","text":"<p>Executes the tool method.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The input state for the node.</p> required <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>The state after executing the tool method.</p> Source code in <code>graphorchestrator\\nodes\\nodes.py</code> <pre><code>async def execute(self, state: State) -&gt; State:\n    \"\"\"\n    Executes the tool method.\n\n    Args:\n        state (State): The input state for the node.\n\n    Returns:\n        State: The state after executing the tool method.\n    \"\"\"\n    log = GraphLogger.get()\n\n    log.info(\n        **wrap_constants(\n            message=\"ToolNode execution started\",\n            **{\n                LC.EVENT_TYPE: \"tool\",\n                LC.NODE_ID: self.node_id,\n                LC.NODE_TYPE: \"ToolNode\",\n                LC.ACTION: \"execute_start\",\n                LC.INPUT_SIZE: len(state.messages),\n            },\n        )\n    )\n\n    result = (\n        await self.func(state)\n        if asyncio.iscoroutinefunction(self.func)\n        else self.func(state)\n    )\n\n    log.info(\n        **wrap_constants(\n            message=\"ToolNode execution completed\",\n            **{\n                LC.EVENT_TYPE: \"tool\",\n                LC.NODE_ID: self.node_id,\n                LC.NODE_TYPE: \"ToolNode\",\n                LC.ACTION: \"execute_end\",\n                LC.OUTPUT_SIZE: len(result.messages),\n            },\n        )\n    )\n\n    return result\n</code></pre>"},{"location":"reference/nodes/ToolSetNode/","title":"ToolSet Node","text":""},{"location":"reference/nodes/ToolSetNode/#graphorchestrator.nodes.nodes.ToolSetNode","title":"<code>graphorchestrator.nodes.nodes.ToolSetNode</code>","text":"<p>               Bases: <code>ProcessingNode</code></p> <p>A ProcessingNode that invokes a remote ToolSetServer endpoint as an HTTP call.</p> <p>Each execution: 1. Sends the current State.messages as JSON to <code>{base_url}/tools/{tool_name}</code>. 2. Parses the JSON response into a new State.</p> Source code in <code>graphorchestrator\\nodes\\nodes.py</code> <pre><code>class ToolSetNode(ProcessingNode):\n    \"\"\"\n    A ProcessingNode that invokes a remote ToolSetServer endpoint as an HTTP call.\n\n    Each execution:\n    1. Sends the current State.messages as JSON to `{base_url}/tools/{tool_name}`.\n    2. Parses the JSON response into a new State.\n    \"\"\"\n\n    httpx = httpx\n\n    def __init__(self, node_id: str, base_url: str, tool_name: str) -&gt; None:\n        self.base_url = base_url.rstrip(\"/\")\n        self.tool_name = tool_name\n        action = self._make_tool_action()\n\n        GraphLogger.get().info(\n            **wrap_constants(\n                message=\"ToolSetNode initialized\",\n                **{\n                    LC.EVENT_TYPE: \"tool\",\n                    LC.NODE_ID: node_id,\n                    LC.NODE_TYPE: \"ToolSetNode\",\n                    LC.ACTION: \"node_created\",\n                    LC.CUSTOM: {\"tool_name\": self.tool_name, \"base_url\": self.base_url},\n                },\n            )\n        )\n\n        super().__init__(node_id, action)\n\n    def _make_tool_action(self) -&gt; Callable[[State], State]:\n        \"\"\"\n        Constructs the @node_action-wrapped coroutine that performs the HTTP call.\n        \"\"\"\n        url = f\"{self.base_url}/tools/{self.tool_name}\"\n\n        @node_action\n        async def _action(state: State) -&gt; State:\n            log = GraphLogger.get()\n\n            log.info(\n                **wrap_constants(\n                    message=\"ToolSetNode HTTP request started\",\n                    **{\n                        LC.EVENT_TYPE: \"tool\",\n                        LC.NODE_ID: self.node_id,\n                        LC.NODE_TYPE: \"ToolSetNode\",\n                        LC.ACTION: \"tool_http_start\",\n                        LC.INPUT_SIZE: len(state.messages),\n                        LC.CUSTOM: {\"url\": url},\n                    },\n                )\n            )\n\n            payload = {\"messages\": state.messages}\n            async with httpx.AsyncClient() as client:\n                resp = await client.post(url, json=payload, timeout=10.0)\n                resp.raise_for_status()\n                data = resp.json()\n                new_state = State(messages=data.get(\"messages\", []))\n\n                log.info(\n                    **wrap_constants(\n                        message=\"ToolSetNode HTTP call succeeded\",\n                        **{\n                            LC.EVENT_TYPE: \"tool\",\n                            LC.NODE_ID: self.node_id,\n                            LC.NODE_TYPE: \"ToolSetNode\",\n                            LC.ACTION: \"tool_http_success\",\n                            LC.OUTPUT_SIZE: len(new_state.messages),\n                            LC.SUCCESS: True,\n                            LC.CUSTOM: {\"url\": url, \"status_code\": resp.status_code},\n                        },\n                    )\n                )\n\n                return new_state\n\n        return _action\n</code></pre>"},{"location":"reference/visualization/Representation/","title":"Representation","text":""},{"location":"reference/visualization/Representation/#graphorchestrator.visualization.representation","title":"<code>graphorchestrator.visualization.representation</code>","text":""},{"location":"reference/visualization/Visualizer/","title":"Visualizer","text":""},{"location":"reference/visualization/Visualizer/#graphorchestrator.visualization.visualizer","title":"<code>graphorchestrator.visualization.visualizer</code>","text":""}]}